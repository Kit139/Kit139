<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>20210124CITPython交流+WebScraper可视化爬虫</title>
      <link href="2021/01/24/20210124CITPython%E4%BA%A4%E6%B5%81+WebScraper%E5%8F%AF%E8%A7%86%E5%8C%96%E7%88%AC%E8%99%AB/"/>
      <url>2021/01/24/20210124CITPython%E4%BA%A4%E6%B5%81+WebScraper%E5%8F%AF%E8%A7%86%E5%8C%96%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="20210124CITPython交流-WebScraper可视化爬虫"><a href="#20210124CITPython交流-WebScraper可视化爬虫" class="headerlink" title="20210124CITPython交流+WebScraper可视化爬虫"></a>20210124CITPython交流+WebScraper可视化爬虫</h1><h2 id="可视化爬虫"><a href="#可视化爬虫" class="headerlink" title="可视化爬虫"></a>可视化爬虫</h2><p>为什么叫可视化爬虫？（当然这里本人承认自己略有根据地随意起的哈哈哈哈哈哈）</p><ul><li><p>本教程可面向纯小白用户，不写代码不写公式，迈出数据分析的第一步。</p></li><li><p>这些操作几乎全部都是GUI可视化界面的操作，当然有点网页结构、爬虫思维更好。</p></li><li><p>生活中很多的数据分析场合，都是很轻量的，不需要上 Python 爬虫、高并发架构，机器学习等重武器，一个浏览器再加一个 Excel 就足够了：</p><p>比如说某门课程论文交稿只有几天了，急需快速爬取数据进行数据分析，这时候临阵磨枪学习 Python 爬虫知识时间完全不够；<br>做一些市场调研和运营工作需要对数据进行采集，让技术部门支持的话，走流程的周期过长，不如撸起袖子自己做；<br>工作跳槽，想知道市场上的技能要求和薪资分布，需要采集数据并分析市场需求；<br>……<br>这些都是生活中会遇到的问题，面对这些数据量不大（100~10000）的分析需求，非互联网技术人士去学习一些编程知识其实性价比并不高。我们不如利用手头最常见的工具——Excel 和 浏览器，去分析去梳理数据，辅助进行思考和更好的决策。</p></li></ul><p>这也算本门教程的目的——用 20% 的精力解决 80% 的数据分析需求，解放个人的生产力。</p><p>爬虫，即数据采集，就是利用爬虫软件从互联网上爬取想要数据，然后存储到本地；简单的可以使用Excel、WebScraper、八爪鱼等，今天我们以WebScraper为例，因为在我眼里，小组的人Excel爬虫都会点儿，而八爪鱼采集器≈浏览器+插件WebScraper！！！</p><p>而且WebScraper还有以下有点：</p><ol><li>门槛足够低，只要你电脑上安装了 Chrome 浏览器就可以用</li><li>永久免费，无付费功能，无需注册</li><li>操作简单，点几次鼠标就能爬取网页，真正意义上的 0 行代码写爬虫</li></ol><h2 id="WebScraper安装"><a href="#WebScraper安装" class="headerlink" title="WebScraper安装"></a>WebScraper安装</h2><p>会科学上网的请点击谷歌商店搜索WebScraper关键词，<a href="https://chrome.google.com/webstore/detail/web-scraper-free-web-scra/jnhgnonknehpejjnehehllkliplmbmhn?hl=zh-CN">直达通道</a></p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62402899.webp" alt="img"></p><p>也可到国内一些类似谷歌商店的镜像，如<a href="https://chrome.zzzmh.cn/">极简插件</a>、<a href="https://www.extfans.com/">扩展密</a>、<a href="https://www.gugeapps.net/">仿chrome 网上应用店</a>等，但因为非官方能科学上网的还是不太建议</p><h2 id="爬取一页"><a href="#爬取一页" class="headerlink" title="爬取一页"></a>爬取一页</h2><p>但凡做爬虫练手，第一个爬取的网站一般都是<a href="https://movie.douban.com/top250?start=0&filter=">豆瓣电影 TOP 250</a>，网址链接是 <a href="https://movie.douban.com/top250?start=0&amp;filter=%E3%80%82%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%B8%8A%E6%89%8B%EF%BC%8C%E6%88%91%E4%BB%AC%E7%88%AC%E5%8F%96%E7%9A%84%E5%86%85%E5%AE%B9%E5%B0%BD%E9%87%8F%E7%AE%80%E5%8D%95%EF%BC%8C%E6%89%80%E4%BB%A5%E6%88%91%E4%BB%AC%E5%8F%AA%E7%88%AC%E5%8F%96%E7%AC%AC%E4%B8%80%E9%A1%B5%E7%9A%84%E7%94%B5%E5%BD%B1%E6%A0%87%E9%A2%98%E3%80%82">https://movie.douban.com/top250?start=0&amp;filter=。第一次上手，我们爬取的内容尽量简单，所以我们只爬取第一页的电影标题。</a></p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697176.webp" alt="img"></p><p>浏览器按 <code>F12</code> 打开控制台，并把控制台放在网页的<strong>下方</strong>（具体操作可以看<a href="https://www.cnblogs.com/web-scraper/p/web_scraper_start_2.html">上一篇文章</a>），然后找到 Web Scraper 这个 Tab，点进去就来到了 Web Scraper 的控制页面。</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697178.webp" alt="img"></p><p>进入 Web Scraper 的控制页面后，我们按照 <code>Create new sitemap</code> -&gt; <code>Create Sitemap</code> 的操作路径，创建一个新的爬虫，<code>sitemap</code> 是啥意思并不重要，你就当他是个爬虫的别名就好了。</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697177.webp" alt="img"></p><p>我们在接下来出现的输入框里依次输入爬虫名和要爬取的链接。</p><p>爬虫名可能会有字符类型的限制，我们看一下规则规避就好了，最后点击 <code>Create Sitemap</code> 这个按钮，创建我们的第一个爬虫。</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697181.webp" alt="img"></p><p>这时候会跳到一个新的操作面板，不要管别的，我们直接点击 <code>Add new selector</code> 这个蓝底白字的按钮，顾名思义，创建一个选择器，用来选择我们想要抓取的元素。</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697180.webp" alt="img"></p><p>这时候就要开始正式的数据抓取环节了！我们先观察一下这个面板有些什么东西：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697183.webp" alt="img"></p><blockquote><p>1.首先有个 Id，这个就是给我们要爬取的内容标注一个 id，因为我们要抓取电影的名字，简单起见就取个 name 吧； 2.电影名字很明显是一段文字，所以 Type 类型肯定是 Text，在这个爬虫工具里，默认 Type 类型就是 Text，这次的爬取工作就不需要改动了； 3.我们把多选按钮 Multiple 勾选上，因为我们要抓的是批量的数据，不勾选的话只能抓取一个； 4.最后我们点击黄色圆圈里的 Select，开始在网页上勾选电影名字；</p></blockquote><p>当你把鼠标移动到网页时，会发现网页上出现了绿色的方块儿，这些方块就是网页的构成元素，当我们点击鼠标时，绿色的方块儿就会变为红色，表示这个元素被选中了：</p><p><img src="https://img2.doubanio.com/view/note/l/public/p62697182.webp" alt="img"></p><p>这时候我们就可以进行我们的抓取工作了。</p><p>我们先选择「<strong>肖生克的救赎</strong>」这个标题，然后再选择「<strong>霸王别姬</strong>」这个标题（<strong>注意：想达到多选的效果，一定要手动选取两个以上的内容</strong>）</p><p>选完这两个标题后，向下拉动网页，你就会发现所有的电影名字都被选中了：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697185.webp" alt="img"></p><p>拉动网页检查一遍，发现所有的电影标题都被选中后，我们就可以点击 <code>Done selecting!</code>这个按钮，表示选择完毕；</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697184.webp" alt="img"></p><p>点击按钮后你会发现下图的红框位置会出现了一些字符，一般出现这个就表示选取成功了：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697186.webp" alt="img"></p><p>我们点击 <code>Data preview</code> 这个按钮，就可以预览我们的抓取效果了：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697187.webp" alt="img"></p><p>没什么问题的话，关闭 Data Preview 弹窗，翻到面板的最下面，有个 <code>Save selector</code> 的蓝色按钮，点击后我们会回退到上一个面板。</p><p>这时候你会发现多了一行数据，其实就是我们刚刚的操作内容被记录下来了。</p><p>在顶部的 tab 栏，有一个 <code>Sitemap top250</code> 的 tab，这个就是我们刚刚创建的爬虫。点击它，再点击下拉菜单里的 <code>Scrape</code> 按钮，开始我们的数据抓取。</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697188.webp" alt="img"></p><p>这时候你会跳到另一个面板，里面有两个输入框，先别管他们是什么，全部输入 2000 就好了。</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697189.webp" alt="img"></p><p>点击 <code>Start scraping</code> 蓝色按钮后，会跳出一个新的网页，<code>Web Scraper</code> 插件会在这里进行数据抓取：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697191.webp" alt="img"></p><p>一般弹出的网页自动关闭就代表着数据抓取结束了，我们点击面板上的 <code>refresh</code> 蓝色按钮，就可以看到我们抓取的数据了！</p><p>在这个预览面板上，第一列是 web scraper 自动添加的编号，没啥意义；第二列是抓取的链接，第三列就是我们抓取的数据了。</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697190.webp" alt="img"></p><p>这个数据会存储在我们的浏览器里，我们也可以点击 <code>Sitemap top250</code> 下的 <code>Export data as CSV</code>，这样就可以导出成 <code>.csv</code> 格式的数据，这种格式可以用 Excel 打开，我们可以用 Excel 做一些数据格式化的操作。</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62697192.webp" alt="img"></p><p>接下来我们分析一下网页进行翻页（其实爬虫第一步应该是先分析网页来着哈哈哈哈哈哈哈）</p><h2 id="分析网页"><a href="#分析网页" class="headerlink" title="分析网页"></a>分析网页</h2><p>我们先看看第一页的豆瓣网址链接：</p><blockquote><p><a href="https://movie.douban.com/top250?start=0&amp;filter=">https://movie.douban.com/top250?start=0&amp;filter=</a></p></blockquote><ol><li><code>https://movie.douban.com</code> 这个很明显就是个豆瓣的电影网址，没啥好说的</li><li><code>top250</code> 这个一看就是网页的内容，豆瓣排名前 250 的电影，也没啥好说的</li><li><code>?</code> 后面有个<code>start=0&amp;filter=</code> ，根据英语提示来看，好像是说筛选（filter），从 0 开始（start）</li></ol><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62890353.webp" alt="img"></p><p>再看看第二页的网址链接，前面都一样，只有后面的参数变了，变成了 <code>start=25</code>，从 25 开始；</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62890355.webp" alt="img"></p><p>我们再看看第三页的链接，参数变成了  <code>start=50</code>，从 50 开始；</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62890356.webp" alt="img"></p><p>分析 3 个链接我们很容易得出规律：</p><blockquote><p>start=0，表示从排名第 1 的电影算起，展示 1-25 的电影 start=25，表示从排名第 26 的电影算起，展示 26-50 的电影 start=50，表示从排名第 51 的电影算起，展示 51-75 的电影 …… start=225，表示从排名第 226 的电影算起，展示 226-250 的电影</p></blockquote><p>规律找到了就好办了，只要技术提供支持就行。<strong>随着深入学习，你会发现 Web Scraper 的操作并不是难点，最需要思考的其实还是这个找规律。</strong></p><p>Web Scraper 针对这种通过<strong>超链接数字分页</strong>获取分页数据的网页，提供了非常便捷的操作，那就是<strong>范围指定器</strong>。</p><p>比如说你想抓取的网页链接是这样的：</p><ul><li><code>http://example.com/page/1</code></li><li><code>http://example.com/page/2</code></li><li><code>http://example.com/page/3</code></li></ul><p>你就可以写成 <a href="http://example.com/page/[1-3]%EF%BC%8C%E6%8A%8A%E9%93%BE%E6%8E%A5%E6%94%B9%E6%88%90%E8%BF%99%E6%A0%B7%EF%BC%8CWeb">http://example.com/page/[1-3]，把链接改成这样，Web</a> Scraper 就会自动抓取这三个网页的内容。</p><p>当然，你也可以写成 <a href="http://example.com/page/[1-100]%EF%BC%8C%E8%BF%99%E6%A0%B7%E5%B0%B1%E5%8F%AF%E4%BB%A5%E6%8A%93%E5%8F%96%E5%89%8D">http://example.com/page/[1-100]，这样就可以抓取前</a> 100 个网页。</p><p>那么像我们之前分析的豆瓣网页呢？它不是从 1 到 100 递增的，而是 0 -&gt; 25 -&gt; 50 -&gt; 75 这样每隔 25 跳的，这种怎么办？</p><ul><li><code>http://example.com/page/0</code></li><li><code>http://example.com/page/25</code></li><li><code>http://example.com/page/50</code></li></ul><p>其实也很简单，这种情况可以用 <code>[0-100:25]</code> 表示，每隔 25 是一个网页，100/25=4，爬取前 4 个网页，放在豆瓣电影的情景下，我们只要把链接改成下面的样子就行了；</p><p><a href="https://movie.douban.com/top250?start=%5B0-225:25%5D&amp;filter=">https://movie.douban.com/top250?start=[0-225:25]&amp;filter=</a></p><p>这样 Web Scraper 就会抓取 TOP250 的所有网页了。</p><h2 id="翻页爬取"><a href="#翻页爬取" class="headerlink" title="翻页爬取"></a>翻页爬取</h2><p>解决了链接的问题，接下来就是如何在 Web Scraper 里修改链接了，很简单，就点击两下鼠标：</p><p>**1.**点击 <code>Stiemaps</code>，在新的面板里点击 ID 为 <code>top250</code> 的这列数据；</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62890357.webp" alt="img"></p><p>**2.**进入新的面板后，找到 <code>Stiemap top250</code> 这个 Tab，点击，再点击下拉菜单里的 <code>Edit metadata</code>；</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62890358.webp" alt="img"></p><p>**3.**修改原来的网址，图中的红框是不同之处：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/p62890359.webp" alt="img"></p><p>修改好了超链接，我们重新抓取网页就好了。操作和<a href="https://www.cnblogs.com/web-scraper/p/web_scraper_first_scrape_douban.html">上文</a>一样，我这里就简单复述一下：</p><ol><li>点击 <code>Sitemap top250</code> 下拉菜单里的 <code>Scrape</code> 按钮</li><li>新的操作面板的两个输入框都输入 2000</li><li>点击 <code>Start scraping</code> 蓝色按钮开始抓取数据</li><li>抓取结束后点击面板上的 <code>refresh</code> 蓝色按钮，检测我们抓取的数据</li></ol><p>如果你操作到这里并抓取成功的话，你会发现数据是全部抓取下来了，但是顺序都是乱的。</p><p><img src="https://img3.doubanio.com/view/note/l/public/p62890360.webp" alt="img"></p><p>我们这里先不管顺序问题，因为这个属于<strong>数据清洗</strong>的内容了，我们现在的专题是<strong>数据抓取</strong>。先把相关的知识点讲完，再攻克下一个知识点，才是更合理的学习方式。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总的来说，WebScraper相较于Python门槛更低、更易上手，但上限有限，也希望WebScraper社区不断改进。</p><p>但是还是要说但是了，这种自动化爬虫对于我们来说日常采集数据的需求还是够用一些了。</p><iframe id="pqvE7xEK-1596373719908" src="https://citpan.herokuapp.com/CITpan/CIT%E5%B0%8F%E7%BB%84%E8%B5%84%E6%BA%90/CIT%E5%9F%B9%E8%AE%AD%E5%9B%9E%E6%94%BE/20210124CITPython%E4%BA%A4%E6%B5%81%2BWebScraper%E5%8F%AF%E8%A7%86%E5%8C%96%E7%88%AC%E8%99%AB.mp4" allowfullscreen="true" data-mediaembed="bilibili" __idm_id__="291592193" style="box-sizing: border-box; outline: 0px; margin: 0px; padding: 0px; font-weight: normal; overflow-wrap: break-word; display: block; width: 660px; height: 330px;"></iframe><p>这个插入的视频好像有点问题，去B站吧-&gt;<a href="https://www.bilibili.com/video/BV1d54y1s7hZ">回放</a>与<a href="https://github.com/Kit139/WebScraper">代码</a>献上</p><p>参考文献：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/88193090">不会写Python代码如何抓取豆瓣电影 Top 250</a></li><li><a href="https://www.douban.com/note/724882625">简易数据分析 04 | Web Scraper 初尝–抓取豆瓣高分电影</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> CIT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> WebScraper </tag>
            
            <tag> 自动化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pandas网页Table表格型数据爬虫+快代理实战</title>
      <link href="2021/01/17/Pandas%E7%BD%91%E9%A1%B5Table%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%E7%88%AC%E8%99%AB+%E5%BF%AB%E4%BB%A3%E7%90%86%E5%AE%9E%E6%88%98/"/>
      <url>2021/01/17/Pandas%E7%BD%91%E9%A1%B5Table%E8%A1%A8%E6%A0%BC%E5%9E%8B%E6%95%B0%E6%8D%AE%E7%88%AC%E8%99%AB+%E5%BF%AB%E4%BB%A3%E7%90%86%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<h1 id="Pandas网页Table表格型数据爬虫-快代理实战"><a href="#Pandas网页Table表格型数据爬虫-快代理实战" class="headerlink" title="Pandas网页Table表格型数据爬虫+快代理实战"></a>Pandas网页Table表格型数据爬虫+快代理实战</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>听说有些刚入门的觉得上次的requests爬虫有点难度？那咱们就来个更简单的——pandas爬虫，虽然它很鸡肋。。。</p><p>优点：对于网页的抓取Table表格型数据专能，代码几行代码就能爬下数据，适合入门</p><p>缺点：用途单一，你想要的数据怎么可能都在表格里呢。。。学着玩玩留作备用吧哈哈哈哈哈哈哈</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20210117180027.png"></p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>pandas是基于NumPy 的一种工具，该工具是为解决数据分析任务而创建的。Pandas 纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。pandas提供了大量能使我们快速便捷地处理数据的函数和方法。本次使用pandas中的pd.read_html()这个函数，功能非常强大，可以轻松实现抓取Table表格型数据。无需掌握正则表达式或者xpath等工具，短短的几行代码就可以将网页数据抓取下来。</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="1-Table表格型数据网页结构"><a href="#1-Table表格型数据网页结构" class="headerlink" title="1.Table表格型数据网页结构"></a>1.Table表格型数据网页结构</h3><p>pandas适合抓取Table表格型数据，那么咱们首先得知道什么样的网页具有Table表格型数据结构(有html基础的大佬可自行跳过这一part)。</p><p>我们先来看个简单的例子。（快捷键F12可快速查看网页的HTML结构）</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/image-20210117220259618.png" alt="image-20210117220259618"></p><p>从以上网站可以看出，数据存储在一个table表格中，thread为表头，tbody为表格数据，tbody中的一个tr对应表中的一行，一个td对应一个表中元素。</p><p>我们再来看一个例子：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/hn2vzm93rz.png" alt="hn2vzm93rz"></p><p>也许你已经发现了规律，以Table结构展示的表格数据，大致的网页结构如下：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;table class=&quot;...&quot; id=&quot;...&quot;&gt;</span><br><span class="line">     &lt;thead&gt;</span><br><span class="line">     &lt;tr&gt;</span><br><span class="line">     &lt;th&gt;...&lt;/th&gt;</span><br><span class="line">     &lt;/tr&gt;</span><br><span class="line">     &lt;/thead&gt;</span><br><span class="line">     &lt;tbody&gt;</span><br><span class="line">        &lt;tr&gt;</span><br><span class="line">            &lt;td&gt;...&lt;/td&gt;</span><br><span class="line">        &lt;/tr&gt;</span><br><span class="line">        &lt;tr&gt;...&lt;/tr&gt;</span><br><span class="line">        &lt;tr&gt;...&lt;/tr&gt;</span><br><span class="line">        ...</span><br><span class="line">        &lt;tr&gt;...&lt;/tr&gt;</span><br><span class="line">        &lt;tr&gt;...&lt;/tr&gt;        </span><br><span class="line">    &lt;/tbody&gt;</span><br><span class="line">&lt;/table&gt;</span><br></pre></td></tr></table></figure><p>只要网页具有以上结构，你就可以尝试用pandas抓取数据。</p><h3 id="2-pandas请求表格数据流程"><a href="#2-pandas请求表格数据流程" class="headerlink" title="2.pandas请求表格数据流程"></a>2.pandas请求表格数据流程</h3><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/image-20210117185828682.png" alt="流程图"></p><p>针对网页结构类似的表格类型数据，pd.read_html()可以将网页上的表格数据都抓取下来，并以DataFrame的形式装在一个list中返回。</p><h3 id="3-pd-read-html语法及参数"><a href="#3-pd-read-html语法及参数" class="headerlink" title="3.pd.read_html语法及参数"></a>3.pd.read_html语法及参数</h3><p>（1）<strong>基本语法</strong>：</p><p>pycharm快捷键可以看到read_html()的参数</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/image-20210117182126073.png" alt="image-20210117182126073"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pandas.read_html(io,match=<span class="string">&#x27;.+&#x27;</span>,flavor=<span class="literal">None</span>,header=<span class="literal">None</span>,index_col=<span class="literal">None</span>,skiprows=<span class="literal">None</span>, attrs=<span class="literal">None</span>,</span><br><span class="line">parse_dates=<span class="literal">False</span>, thousands=<span class="string">&#x27;, &#x27;</span>, encoding=<span class="literal">None</span>, decimal=<span class="string">&#x27;.&#x27;</span>, converters=<span class="literal">None</span>, na_values=<span class="literal">None</span>, </span><br><span class="line">keep_default_na=<span class="literal">True</span>, displayed_only=<span class="literal">True</span>）</span><br></pre></td></tr></table></figure><p>（2）<strong>主要参数</strong>：</p><table><thead><tr><th>参数</th><th>释义</th></tr></thead><tbody><tr><td>io</td><td>接收网址、文件、字符串</td></tr><tr><td>parse_dates</td><td>解析日期</td></tr><tr><td>flavor</td><td>解析器</td></tr><tr><td>header</td><td>标题行</td></tr><tr><td>skiprows</td><td>跳过的行属性，比如 attrs = {‘id’: ‘table’}</td></tr></tbody></table><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><h3 id="案例1：抓取世界大学排名"><a href="#案例1：抓取世界大学排名" class="headerlink" title="案例1：抓取世界大学排名"></a>案例1：抓取世界大学排名</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line">url1 = <span class="string">&#x27;http://www.compassedu.hk/qs&#x27;</span></span><br><span class="line">df1 = pd.read_html(url1)[<span class="number">0</span>]  <span class="comment">#0表示网页中的第一个Table</span></span><br><span class="line">df1.to_csv(<span class="string">&#x27;世界大学综合排名.csv&#x27;</span>,index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>没错，5行代码，几秒钟就搞定，我们来预览下爬取到的数据：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/24171048-3c6ae3eff36aecb7.png" alt="img"></p><h3 id="案例2：抓取新浪财经基金重仓股数据"><a href="#案例2：抓取新浪财经基金重仓股数据" class="headerlink" title="案例2：抓取新浪财经基金重仓股数据"></a>案例2：抓取新浪财经基金重仓股数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line">df2 = pd.DataFrame()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    url2 = <span class="string">&#x27;http://vip.stock.finance.sina.com.cn/q/go.php/vComStockHold/kind/jjzc/index.phtml?p=&#123;page&#125;&#x27;</span>.<span class="built_in">format</span>(page=i+<span class="number">1</span>)</span><br><span class="line">    df2 = pd.concat([df2,pd.read_html(url2)[<span class="number">0</span>]])</span><br><span class="line">    print(<span class="string">&#x27;第&#123;page&#125;页抓取完成&#x27;</span>.<span class="built_in">format</span>(page = i + <span class="number">1</span>))</span><br><span class="line">df2.to_csv(<span class="string">&#x27;./新浪财经数据.csv&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>,index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>没错，8行代码搞定，还是那么简单。如果对翻页爬虫不理解，可查看公众号「菜J学Python」历史原创文章「实战|手把手教你用Python爬虫(附详细源码)」，如果对DataFrame合并不理解，可查看公众号历史原创文章「基础|Pandas常用知识点汇总(四)」。</p><p>我们来预览下爬取到的数据：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/24171048-b5db3843aae2256d.png" alt="img"></p><h3 id="案例3：抓取证监会披露的IPO数据"><a href="#案例3：抓取证监会披露的IPO数据" class="headerlink" title="案例3：抓取证监会披露的IPO数据"></a>案例3：抓取证监会披露的IPO数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> DataFrame</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.time() <span class="comment">#程序计时</span></span><br><span class="line">df3 = DataFrame(data=<span class="literal">None</span>,columns=[<span class="string">&#x27;公司名称&#x27;</span>,<span class="string">&#x27;披露类型&#x27;</span>,<span class="string">&#x27;上市板块&#x27;</span>,<span class="string">&#x27;保荐机构&#x27;</span>,<span class="string">&#x27;披露时间&#x27;</span>, <span class="string">&#x27;公告&#x27;</span>]) <span class="comment">#添加列名</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">3</span>):</span><br><span class="line">    url3 =<span class="string">&#x27;http://eid.csrc.gov.cn/ipo/1010/index_%s.html&#x27;</span>%<span class="built_in">str</span>(i)</span><br><span class="line">    print(url3)</span><br><span class="line">    df3_1 = pd.read_html(url3,encoding=<span class="string">&#x27;utf-8&#x27;</span>)[<span class="number">0</span>]  <span class="comment">#必须加utf-8，否则乱码</span></span><br><span class="line">    df3_1.columns=[<span class="string">&#x27;公司名称&#x27;</span>,<span class="string">&#x27;披露类型&#x27;</span>,<span class="string">&#x27;上市板块&#x27;</span>,<span class="string">&#x27;保荐机构&#x27;</span>,<span class="string">&#x27;披露时间&#x27;</span>, <span class="string">&#x27;公告&#x27;</span>] <span class="comment">#新的df添加列名</span></span><br><span class="line">    df3 = pd.concat([df3,df3_1])  <span class="comment">#数据合并</span></span><br><span class="line">    print(<span class="string">&#x27;第&#123;page&#125;页抓取完成&#x27;</span>.<span class="built_in">format</span>(page=i))</span><br><span class="line"><span class="comment"># df3.to_csv(&#x27;./上市公司IPO信息.csv&#x27;, encoding=&#x27;utf-8&#x27;,index=0) #保存数据到csv文件</span></span><br><span class="line">end = time.time()</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;共抓取&#x27;</span>,<span class="built_in">len</span>(df3),<span class="string">&#x27;家公司,&#x27;</span> + <span class="string">&#x27;用时&#x27;</span>,<span class="built_in">round</span>((end-start)/<span class="number">60</span>,<span class="number">2</span>),<span class="string">&#x27;分钟&#x27;</span>)</span><br><span class="line">print(df3)</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/image-20210117222007162.png" alt="image-20210117222007162"></p><p>这里只爬取了两页，当然你将range里改一下就能爬取全部页数了，可能需要time.sleep()一下哈哈哈哈哈哈。另外可以发现添加了个程序计时，方便查看爬取速度。</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/24171048-fe0161c35ad75967.png" alt="img"></p><p>2分14秒爬下217页4334条数据，相当nice了。我们来预览下爬取到的数据：</p><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/24171048-9c753186a310ef20.png" alt="img"></p><p>需要注意的是，并不是所有表格都可以用pd.read_html爬取，有的网站表面上看起来是表格，但在网页源代码中不是table格式，而是list列表格式。还有一种情况是网页前后端分离时，你就要XHR或者JS分析了哈哈哈哈哈哈哈</p><h3 id="案例4：快代理"><a href="#案例4：快代理" class="headerlink" title="案例4：快代理"></a>案例4：快代理</h3><p>见视频<a href="https://www.bilibili.com/video/BV15K4y1W7KX/">回放</a></p><p>演示代码见<a href="https://github.com/Kit139/Pandas.read_html">github</a>上</p><p>tips：</p><blockquote><p>小马哥：你们回去都下载下来试试哈，这东西还挺好玩的</p><p>哈哈哈哈哈哈哈还不赶快去下载eNSP学<a href="https://www.bilibili.com/video/BV1Dr4y1T7Co/">网络基础</a>去</p></blockquote><p>参考文章：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/122361747">天秀！Pandas还能用来写爬虫？</a></li><li><a href="https://www.jianshu.com/p/ecb4b2fc7f81">Pandas也能爬虫，还如此简单！</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> Pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python爬取股吧评论+情感分析</title>
      <link href="2021/01/15/Python%E7%88%AC%E5%8F%96%E8%82%A1%E5%90%A7%E8%AF%84%E8%AE%BA+%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
      <url>2021/01/15/Python%E7%88%AC%E5%8F%96%E8%82%A1%E5%90%A7%E8%AF%84%E8%AE%BA+%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h2 id="Python爬取股吧评论-情感分析"><a href="#Python爬取股吧评论-情感分析" class="headerlink" title="Python爬取股吧评论+情感分析"></a>Python爬取股吧评论+情感分析</h2><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>上学期老师就让我爬取东方财富股吧评论进行情感打分了，这里总结一下。</p><p>这里只弄简单的，实际短时间爬取海量数据还要买代理。。。 </p><h2 id="2-爬取工具"><a href="#2-爬取工具" class="headerlink" title="2. 爬取工具"></a>2. 爬取工具</h2><p>本文使用了 Python 的 <code>request</code> 库作为主要爬取工具，并且该库具有简单易用等特点，能够满足一般的数据爬取需求。</p><p>进一步，本文使用了 <code>xpath</code> 来获取特定标签所储存的信息。<code>XPath</code>，全称 XML Path Language，即 XML 路径语言。<code>XPath</code> 最初设计是用来搜寻 XML 文档的，但是也同样适用于 HTML 文档的搜索。XPath 的选择功能十分强大，不但提供了非常简洁明了的路径选择表达式，而且还提供了超过 100 个内建函数用于字符串、数值、时间的匹配，以及节点、序列的处理等。甚至，我们可以认为几乎所有定位的节点都可以用 <code>XPath</code> 来选择。</p><p> </p><h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3. 代码实现"></a>3. 代码实现</h2><p>本文爬取的股吧为上证指数，作为国民关注度最高的指数，该股吧也是众多的股吧中活跃度最高的。因此用上证指数股吧，作为爬取对象。</p><p>在本文中，我们将仅以「上证指数」的股吧评论为例进行演示。</p><h3 id="3-1-导入相关库"><a href="#3-1-导入相关库" class="headerlink" title="3.1 导入相关库"></a>3.1 导入相关库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import requests         ##获取网页  </span><br><span class="line">from lxml import etree  ##解析文档  </span><br><span class="line">import pandas as pd     ##保存文件  </span><br></pre></td></tr></table></figure><h3 id="3-2-分析网址规律"><a href="#3-2-分析网址规律" class="headerlink" title="3.2 分析网址规律"></a>3.2 分析网址规律</h3><p>网址：<a href="http://guba.eastmoney.com/list,zssh000001,f.html">http://guba.eastmoney.com/list,zssh000001,f.html</a></p><p>第一页如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/640" alt="img"></p><p>第二页如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/guba1.webp" alt="img"></p><p>可以发现，两页网址区别在于 f_ 后面的数字不同，因此可以通过设置数字爬取不同页面内容。</p><h3 id="3-3-爬取和解析网页源代码"><a href="#3-3-爬取和解析网页源代码" class="headerlink" title="3.3 爬取和解析网页源代码"></a>3.3 爬取和解析网页源代码</h3><p><strong>获取 User-Agent</strong></p><p>请求头反爬很常见，尤其是User-Agent现在已是爬虫必备：User-Agent会告诉网站服务器，访问者是通过什么工具来请求的，如果是爬虫请求，一般会拒绝，如果是用户浏览器，就会应答。</p><p>进入开发者模型，点击 Network，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/guba2.png" alt="img"></p><p>然后，点击任意一个 Name 列的标题，就可以看到 <strong>User-Agent</strong>，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/6g435uba4.png" alt="img"></p><p>通过 <code>requests</code> 库，我们可以获取网页源代码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">headers &#x3D; &#123;&#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;85.0.4183.102 Safari&#x2F;537.36&#39;&#125;  #构造头文件，模拟浏览器。</span><br><span class="line">for page in range(1,max_page+1):</span><br><span class="line">    #获取网页源代码</span><br><span class="line">    print(&#39;crawling the page is &#123;&#125;&#39;.format(page))  </span><br><span class="line">    url&#x3D; f&#39;http:&#x2F;&#x2F;guba.eastmoney.com&#x2F;list,zssh000001,f_&#123;page&#125;.html&#39;  </span><br><span class="line">    response  &#x3D; requests.get(url, headers&#x3D;headers) </span><br></pre></td></tr></table></figure><p>然后，通过 <code>xpath</code> 解析网页源代码，我们就可以获取需要信息。</p><p>在谷歌浏览器内按 F12 进入开发者模型，审查我们所需要的元素，如下图:</p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/dhuih.png" alt="img"></p><p>可以看出，所有的标题和时间都保存在属性为 <code>articleh normal_post</code> 的 <code>div</code> 标签下，因此我们可以构造如下代码进行爬取。当然，以上过程也可以借助 <code>XPath Helper</code> 工具大大简化，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">title &#x3D; root.xpath(&quot;&#x2F;&#x2F;div[contains(@class,&#39;articleh normal_post&#39;)]&#x2F;&#x2F;span[@class&#x3D;&#39;l3 a3&#39;]&#x2F;&#x2F;a&#x2F;&#x2F;text()&quot;)  </span><br><span class="line">time &#x3D; root.xpath(&quot;&#x2F;&#x2F;div[contains(@class,&#39;articleh normal_post&#39;)]&#x2F;&#x2F;span[@class&#x3D;&#39;l5 a5&#39;]&#x2F;&#x2F;text()&quot;)  </span><br></pre></td></tr></table></figure><p>完整代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">max_page  &#x3D; 20   #最大爬取页面</span><br><span class="line">all_title &#x3D; []   #爬取的标题存储列表</span><br><span class="line">all_time  &#x3D; []   #爬取的发表时间储存列表</span><br><span class="line">headers &#x3D; &#123;&#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;85.0.4183.102 Safari&#x2F;537.36&#39;&#125;  #构造头文件，模拟浏览器。</span><br><span class="line">for page in range(1,max_page+1):</span><br><span class="line">    #获取网页源代码</span><br><span class="line">    print(&#39;crawling the page is &#123;&#125;&#39;.format(page))  </span><br><span class="line">    url&#x3D; f&#39;http:&#x2F;&#x2F;guba.eastmoney.com&#x2F;list,zssh000001,f_&#123;page&#125;.html&#39;  </span><br><span class="line">    response  &#x3D; requests.get(url, headers&#x3D;headers) </span><br><span class="line">    #解析网页源代码</span><br><span class="line">    root &#x3D; etree.HTML(response.text)  </span><br><span class="line">    title &#x3D; root.xpath(&quot;&#x2F;&#x2F;div[contains(@class,&#39;articleh normal_post&#39;)]&#x2F;&#x2F;span[@class&#x3D;&#39;l3 a3&#39;]&#x2F;&#x2F;a&#x2F;&#x2F;text()&quot;)  </span><br><span class="line">    time &#x3D; root.xpath(&quot;&#x2F;&#x2F;div[contains(@class,&#39;articleh normal_post&#39;)]&#x2F;&#x2F;span[@class&#x3D;&#39;l5 a5&#39;]&#x2F;&#x2F;text()&quot;)  </span><br><span class="line">    all_title +&#x3D; title  #保存到总数组上</span><br><span class="line">    all_time  +&#x3D; time </span><br></pre></td></tr></table></figure><h3 id="3-4-保存结果"><a href="#3-4-保存结果" class="headerlink" title="3.4 保存结果"></a>3.4 保存结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_raw &#x3D; pd.DataFrame()  </span><br><span class="line">data_raw[&#39;title&#39;] &#x3D; all_title  </span><br><span class="line">data_raw[&#39;time&#39;] &#x3D; all_time  </span><br><span class="line">data_raw.to_excel(&#39;.&#x2F;&#x2F;data_raw.xlsx&#39;, index&#x3D;False)  </span><br></pre></td></tr></table></figure><p>输出结果如下：</p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/huilafh.png" alt="img"></p><p> </p><h2 id="4-情绪打分"><a href="#4-情绪打分" class="headerlink" title="4. 情绪打分"></a>4. 情绪打分</h2><p>不久之前，百度正式发布情感预训练模型 SKEP (Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis)。通过利用情感知识增强预训练模型，SKEP 在 14 项中英情感分析典型任务上全面超越 SOTA。</p><p>具体实现原理，详见「SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis」。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#!pip install paddlehub</span><br><span class="line"></span><br><span class="line">import paddlehub as hub</span><br><span class="line">data_raw &#x3D; pd.read_excel(&quot;.\\data_raw.xlsx&quot;)</span><br><span class="line">data_raw[&#39;time&#39;] &#x3D; pd.to_datetime(&#39;2020 &#39;+data_raw[&#39;time&#39;])</span><br><span class="line">##这里使用了百度开源的成熟NLP模型来预测情感倾向</span><br><span class="line">senta &#x3D; hub.Module(name&#x3D;&quot;senta_bilstm&quot;)</span><br><span class="line">texts &#x3D; data_raw[&#39;title&#39;].tolist()</span><br><span class="line">input_data &#x3D; &#123;&#39;text&#39;:texts&#125;</span><br><span class="line">res &#x3D; senta.sentiment_classify(data&#x3D;input_data)</span><br><span class="line">data_raw[&#39;pos_p&#39;] &#x3D; [x[&#39;positive_probs&#39;] for x in res]</span><br><span class="line">##重采样至五分钟</span><br><span class="line">data_raw.index &#x3D; data_raw[&#39;time&#39;]</span><br><span class="line">data &#x3D; data_raw.resample(&#39;15min&#39;).mean().reset_index()</span><br></pre></td></tr></table></figure><p>部分股本评论的情感评分如下：</p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/kiohfl.png" alt="img"></p><p>可以看出，上述情感评分具有一定借鉴意义。</p><p> </p><h2 id="5-获取上证指数分时数据"><a href="#5-获取上证指数分时数据" class="headerlink" title="5. 获取上证指数分时数据"></a>5. 获取上证指数分时数据</h2><p><code>AkShare</code> 是基于 Python 的财经数据接口库，可以实现对股票、期货、期权、基金、外汇、债券、指数、数字货币等金融产品的基本面数据、历史行情数据、以及衍生数据的快速采集和清洗。接下来，我们将使用 <code>AKShare</code> 库获取上证指数分时数据，具体代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#pip instasll akshare --usre</span><br><span class="line">import akshare as ak</span><br><span class="line">sz_index &#x3D; ak.stock_zh_a_minute(symbol&#x3D;&#39;sh000001&#39;, period&#x3D;&#39;15&#39;, adjust&#x3D;&quot;qfq&quot;)</span><br><span class="line">sz_index[&#39;day&#39;] &#x3D; pd.to_datetime(sz_index[&#39;day&#39;])</span><br><span class="line">sz_index[&#39;close&#39;] &#x3D; sz_index[&#39;close&#39;].astype(&#39;float&#39;)</span><br><span class="line">data &#x3D; data.merge(sz_index,left_on&#x3D;&#39;time&#39;,right_on&#x3D;&#39;day&#39;,how&#x3D;&#39;inner&#39;)</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">matplotlib.use(&#39;Qt5Agg&#39;)</span><br><span class="line">data.index &#x3D; data[&#39;time&#39;]</span><br><span class="line">data[[&#39;pos_p&#39;,&#39;close&#39;]].plot(secondary_y&#x3D;[&#39;close&#39;])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/%E5%93%88%E9%A3%9E%E8%B7%AF%E5%AE%9D%EF%BC%8Csjbgb.png" alt="img"></p><p>可以看出，情绪相对于上证指数存在一个滞后效应。在初始的大幅上涨中，情绪没有立刻上涨，而是在第二次小幅上涨后才出现大幅度的上升。</p><p> </p><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><p>总的来说不难，但步骤很丰富，在完善完善就是一个中等项目了哈哈哈哈哈</p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
            <tag> 情感分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python爬虫入门首选，Requests+正则/Xpath</title>
      <link href="2021/01/10/Python%E5%85%A5%E9%97%A8%E9%A6%96%E9%80%89%EF%BC%8CRequests%20%E5%BA%93%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
      <url>2021/01/10/Python%E5%85%A5%E9%97%A8%E9%A6%96%E9%80%89%EF%BC%8CRequests%20%E5%BA%93%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="Python爬虫入门首选，Requests-正则-Xpath"><a href="#Python爬虫入门首选，Requests-正则-Xpath" class="headerlink" title="Python爬虫入门首选，Requests+正则/Xpath"></a>Python爬虫入门首选，Requests+正则/Xpath</h1><iframe id="pqvE7xEK-1596373719908" src="https://player.bilibili.com/player.html?aid=756043858" allowfullscreen="true" data-mediaembed="bilibili" __idm_id__="291592193" style="box-sizing: border-box; outline: 0px; margin: 0px; padding: 0px; font-weight: normal; overflow-wrap: break-word; display: block; width: 660px; height: 330px;"></iframe><p>学习完Python基本知识，我们就可以正式步入Python 爬虫的大门。</p><p>学习爬虫，最基础的便是模拟浏览器向服务器发出请求，那么我们需要从什么地方做起呢？请求需要我们自己来构造吗？需要关心请求这个数据结构的实现吗？需要了解 HTTP、TCP、IP 层的网络传输通信吗？需要知道服务器的响应和应答原理吗？</p><p>可能你无从下手，不过不用担心，Python 的强大之处就是提供了功能齐全的类库来帮助我们完成这些请求。利用 Python 现有的库我们可以非常方便地实现网络请求的模拟，常见的库有 urllib、requests 等。</p><p>拿 requests 这个库来说，有了它，我们只需要关心请求的链接是什么，需要传的参数是什么，以及如何设置可选的参数就好了，不用深入到底层去了解它到底是怎样传输和通信的。有了它，两行代码就可以完成一个请求和响应的处理过程，非常方便地得到网页内容。</p><p>接下来，就让我们用 Python 的 requests 库开始我们的爬虫之旅吧。</p><p><a href="https://requests.readthedocs.io/en/master/">Requests官方文档</a>   <a href="https://requests.readthedocs.io/zh_CN/latest/">中文文档</a></p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>首先，requests 库是 Python 的一个第三方库，不是自带的。所以我们需要额外安装（<strong>Anaconda自带丰富库，可跳过此步</strong>）。</p><p>在这之前需要你先安装好 Python3 环境，如 Python 3.8 版本。</p><p>安装好 Python3 之后，我们使用 pip3 即可轻松地安装好 requests 库：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install requests</span><br></pre></td></tr></table></figure><p>由于科学上网的原因，国外的第三方库官方下载龟速，请pip换源建议清华镜像。</p><p>安装完成之后，我们就可以开始我们的网络爬虫之旅了。</p><h3 id="实例引入"><a href="#实例引入" class="headerlink" title="实例引入"></a>实例引入</h3><p>用 Python 写爬虫的第一步就是模拟发起一个请求，把网页的源代码获取下来。</p><p>当我们在浏览器中输入一个 URL 并回车，实际上就是让浏览器帮我们发起一个 GET 类型的 HTTP 请求，浏览器得到源代码后，把它渲染出来就可以看到网页内容了。</p><p>那如果我们想用 requests 来获取源代码，应该怎么办呢？很简单，requests 这个库提供了一个 get 方法，我们调用这个方法，并传入对应的 URL 就能得到网页的源代码。</p><p>比如这里有一个示例网站：<a href="https://zhangkaiheng.gitee.io/%EF%BC%8C%E5%85%B6%E5%86%85%E5%AE%B9%E5%A6%82%E4%B8%8B%EF%BC%9A">https://zhangkaiheng.gitee.io/，其内容如下：</a></p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/20210102221113.png"></p><p>哈哈哈哈哈哈！竟然到了爬自己博客的地步！！！</p><p>第一步当然就是获取它的网页源代码了。</p><p>我们可以用 requests 这个库轻松地完成这个过程，代码的写法是这样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import requests  </span><br><span class="line"></span><br><span class="line">r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;zhangkaiheng.gitee.io&#x2F;&#39;)  </span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html lang&#x3D;&quot;zh-CN&quot; data-theme&#x3D;&quot;light&quot;&gt;</span><br><span class="line"></span><br><span class="line">&lt;head&gt;</span><br><span class="line">    &lt;meta charset&#x3D;&quot;UTF-8&quot;&gt;</span><br><span class="line">    &lt;meta http-equiv&#x3D;&quot;X-UA-Compatible&quot; content&#x3D;&quot;IE&#x3D;edge&quot;&gt;</span><br><span class="line">    &lt;meta name&#x3D;&quot;viewport&quot; content&#x3D;&quot;width&#x3D;device-width,initial-scale&#x3D;1&quot;&gt;</span><br><span class="line">    &lt;title&gt;爱爬虫与大数据&lt;&#x2F;title&gt;</span><br><span class="line">    &lt;meta name&#x3D;&quot;keywords&quot; content&#x3D;&quot;爱爬虫与大数据&quot;&gt;</span><br><span class="line">    &lt;meta name&#x3D;&quot;author&quot; content&#x3D;&quot;Kit&quot;&gt;</span><br><span class="line">    &lt;meta name&#x3D;&quot;copyright&quot; content&#x3D;&quot;Kit&quot;&gt;</span><br><span class="line">    ...</span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line"></span><br><span class="line">&lt;body&gt;</span><br><span class="line">    &lt;div id&#x3D;&quot;sidebar&quot;&gt;</span><br><span class="line">        &lt;div id&#x3D;&quot;menu-mask&quot;&gt;&lt;&#x2F;div&gt;</span><br><span class="line">        &lt;div id&#x3D;&quot;sidebar-menus&quot;&gt;</span><br><span class="line">...  </span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure><p>由于网页内容比较多，这里省略了大部分内容。</p><p>不过看运行结果，我们已经成功获取网页的 HTML 源代码，里面包含了电影的标题、类型、上映时间，等等。把网页源代码获取下来之后，下一步我们把想要的数据提取出来，数据的爬取就完成了。</p><p>这个实例的目的是让你体会一下 requests 这个库能帮我们实现什么功能。我们仅仅用 requests 的 get 方法就成功发起了一个 GET 请求，把网页源代码获取下来了，是不是很方便呢？</p><h3 id="请求"><a href="#请求" class="headerlink" title="请求"></a>请求</h3><p>HTTP 中最常见的请求之一就是 GET 请求，下面我们来详细了解利用 requests 库构建 GET 请求的方法。</p><h4 id="GET-请求"><a href="#GET-请求" class="headerlink" title="GET 请求"></a>GET 请求</h4><p>我们换一个示例网站，其 URL 为 <a href="http://httpbin.org/get%EF%BC%8C%E5%A6%82%E6%9E%9C%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E8%B5%B7%E7%9A%84%E6%98%AF">http://httpbin.org/get，如果客户端发起的是</a> GET 请求的话，该网站会判断并返回相应的请求信息，包括 Headers、IP 等。</p><p>我们还是用相同的方法来发起一个 GET 请求，代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import requests  </span><br><span class="line"></span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;)  </span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;,</span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;*&#x2F;*&quot;,</span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,</span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;,</span><br><span class="line">    &quot;User-Agent&quot;: &quot;python-requests&#x2F;2.24.0&quot;,</span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root&#x3D;1-5ff98684-119fbcba4197a62114bac501&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;origin&quot;: &quot;112.32.241.87&quot;,</span><br><span class="line">  &quot;url&quot;: &quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以发现，我们成功发起了 GET 请求，也通过这个网站的返回结果得到了请求所携带的信息，包括 Headers、URL、IP，等等。</p><p>对于 GET 请求，我们知道 URL 后面是可以跟上一些参数的，如果我们现在想添加两个参数，其中 name 是 kit，age 是 25，URL 就可以写成如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;httpbin.org&#x2F;get?name&#x3D;kit&amp;age&#x3D;19</span><br></pre></td></tr></table></figure><p>要构造这个请求链接，是不是要直接写成这样呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get?name&#x3D;kit&amp;age&#x3D;19&#39;)</span><br></pre></td></tr></table></figure><p>这样也可以，但如果这些参数还需要我们手动拼接，未免有点不人性化。</p><p>一般情况下，这种信息我们利用 params 这个参数就可以直接传递了，示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import requests  </span><br><span class="line"></span><br><span class="line">data &#x3D; &#123;  </span><br><span class="line">    &#39;name&#39;: &#39;kit&#39;,  </span><br><span class="line">    &#39;age&#39;: 19</span><br><span class="line">&#125; </span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;, params&#x3D;data)  </span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;</span><br><span class="line">    &quot;age&quot;: &quot;19&quot;, </span><br><span class="line">    &quot;name&quot;: &quot;kit&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;*&#x2F;*&quot;, </span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, </span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;, </span><br><span class="line">    &quot;User-Agent&quot;: &quot;python-requests&#x2F;2.24.0&quot;, </span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root&#x3D;1-5ff99fab-4dc3d0207d4376fd6923f0c4&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;origin&quot;: &quot;112.32.241.87&quot;, </span><br><span class="line">  &quot;url&quot;: &quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get?name&#x3D;kit&amp;age&#x3D;19&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这里我们把 URL 参数通过字典的形式传给 get 方法的 params 参数，通过返回信息我们可以判断，请求的链接自动被构造成了：<a href="http://httpbin.org/get?name=kit&amp;age=19%EF%BC%8C%E8%BF%99%E6%A0%B7%E6%88%91%E4%BB%AC%E5%B0%B1%E4%B8%8D%E7%94%A8%E5%86%8D%E5%8E%BB%E8%87%AA%E5%B7%B1%E6%9E%84%E9%80%A0">http://httpbin.org/get?name=kit&amp;age=19，这样我们就不用再去自己构造</a> URL 了，非常方便。</p><p>另外，网页的返回类型实际上是 str 类型，但是它很特殊，是 JSON 格式的。所以，如果想直接解析返回结果，得到一个 JSON 格式的数据的话，可以直接调用 json 方法。</p><p>示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import requests  </span><br><span class="line"></span><br><span class="line">r &#x3D; requests.get(&#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;)  </span><br><span class="line">print(type(r.text))  </span><br><span class="line">print(r.json())  </span><br><span class="line">print(type(r.json()))</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &#39;str&#39;&gt;</span><br><span class="line">&#123;&#39;args&#39;: &#123;&#125;, &#39;headers&#39;: &#123;&#39;Accept&#39;: &#39;*&#x2F;*&#39;, &#39;Accept-Encoding&#39;: &#39;gzip, deflate&#39;, &#39;Host&#39;: &#39;httpbin.org&#39;, &#39;User-Agent&#39;: &#39;python-requests&#x2F;2.24.0&#39;, &#39;X-Amzn-Trace-Id&#39;: </span><br><span class="line">&#39;Root&#x3D;1-5ff987e4-1248c4cd04158eed34ca44c5&#39;&#125;, &#39;origin&#39;: &#39;112.32.241.87&#39;, &#39;url&#39;: &#39;http:&#x2F;&#x2F;httpbin.org&#x2F;get&#39;&#125;</span><br><span class="line">&lt;class &#39;dict&#39;&gt;</span><br></pre></td></tr></table></figure><p>可以发现，调用 json 方法，就可以将返回结果是 JSON 格式的字符串转化为字典。</p><p>但需要注意的是，如果返回结果不是 JSON 格式，便会出现解析错误，抛出 json.decoder.JSONDecodeError 异常。</p><h4 id="解析网页"><a href="#解析网页" class="headerlink" title="解析网页"></a>解析网页</h4><p>上面的请求链接返回的是 JSON 形式的字符串，那么如果请求普通的网页，则肯定能获得相应的内容了。下面以本课时最初的实例页面为例，我们再加上一点提取信息的逻辑，将代码完善成如下的样子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;maoyan.com&#x2F;board&#x2F;4?offset&#x3D;0&#39;)</span><br><span class="line">pattern &#x3D; re.compile(&#39;&lt;p class&#x3D;&quot;name&quot;&gt;&lt;a href&#x3D;&quot;&#x2F;films&#x2F;.*?&gt;(.*?)&lt;&#x2F;a&gt;&#39;, re.S)</span><br><span class="line">titles &#x3D; re.findall(pattern, r.text)</span><br><span class="line">print(titles)</span><br></pre></td></tr></table></figure><p>在这个例子中我们用到了最基础的正则表达式来匹配出所有的标题。关于正则表达式的相关内容，我们会在下一课时详细介绍，这里作为实例来配合讲解。</p><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#39;我不是药神&#39;, &#39;肖申克的救赎&#39;, &#39;绿皮书&#39;, &#39;海上钢琴师&#39;, &#39;哪吒之魔童降世&#39;, &#39;小偷家族&#39;, &#39;霸王别姬&#39;, &#39;美丽人生&#39;, &#39;盗梦空间&#39;, &#39;这个杀手不太冷&#39;]</span><br></pre></td></tr></table></figure><p>我们发现，这里成功提取出了所有的电影标题。一个最基本的抓取和提取流程就完成了。</p><h4 id="添加-headers"><a href="#添加-headers" class="headerlink" title="添加 headers"></a>添加 headers</h4><p>我们知道，在发起一个 HTTP 请求的时候，会有一个请求头 Request Headers，那么这个怎么来设置呢？</p><p>很简单，我们使用 headers 参数就可以完成了。</p><p>在刚才的实例中，实际上我们是没有设置 Request Headers 信息的，如果不设置，某些网站会发现这不是一个正常的浏览器发起的请求，网站可能会返回异常的结果，导致网页抓取失败。</p><p>要添加 Headers 信息，比如我们这里想添加一个 User-Agent 字段，我们可以这么来写：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers &#x3D; &#123;</span><br><span class="line">    &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;87.7.36.5901 Safari&#x2F;537.36&#39;</span><br><span class="line">&#125;</span><br><span class="line">r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;zhangkaiheng.gitee.io&#x2F;&#39;, headers&#x3D;headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>当然，我们可以在 headers 这个参数中任意添加其他的字段信息。</p><h4 id="POST-请求"><a href="#POST-请求" class="headerlink" title="POST 请求"></a>POST 请求</h4><p>前面我们了解了最基本的 GET 请求，另外一种比较常见的请求方式是 POST。使用 requests 实现 POST 请求同样非常简单，示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">data &#x3D; &#123;&#39;name&#39;: &#39;kit&#39;, &#39;age&#39;: &#39;20&#39;&#125;</span><br><span class="line">r &#x3D; requests.post(&quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;, data&#x3D;data)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>这里还是请求 <a href="http://httpbin.org/post%EF%BC%8C%E8%AF%A5%E7%BD%91%E7%AB%99%E5%8F%AF%E4%BB%A5%E5%88%A4%E6%96%AD%E5%A6%82%E6%9E%9C%E8%AF%B7%E6%B1%82%E6%98%AF">http://httpbin.org/post，该网站可以判断如果请求是</a> POST 方式，就把相关请求信息返回。</p><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;, </span><br><span class="line">  &quot;data&quot;: &quot;&quot;, </span><br><span class="line">  &quot;files&quot;: &#123;&#125;, </span><br><span class="line">  &quot;form&quot;: &#123;</span><br><span class="line">    &quot;age&quot;: &quot;20&quot;, </span><br><span class="line">    &quot;name&quot;: &quot;kit&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;*&#x2F;*&quot;, </span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, </span><br><span class="line">    &quot;Content-Length&quot;: &quot;15&quot;, </span><br><span class="line">    &quot;Content-Type&quot;: &quot;application&#x2F;x-www-form-urlencoded&quot;,         </span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;, </span><br><span class="line">    &quot;User-Agent&quot;: &quot;python-requests&#x2F;2.24.0&quot;, </span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root&#x3D;1-5ff99ef7-1d4426a80c49df9d31ff3250&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;json&quot;: null, </span><br><span class="line">  &quot;origin&quot;: &quot;112.32.241.87&quot;, </span><br><span class="line">  &quot;url&quot;: &quot;http:&#x2F;&#x2F;httpbin.org&#x2F;post&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以发现，我们成功获得了返回结果，其中 form 部分就是提交的数据，这就证明 POST 请求成功发送了。</p><h3 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h3><p>发送请求后，得到的自然就是响应，即 Response。</p><p>在上面的实例中，我们使用 text 和 content 获取了响应的内容。此外，还有很多属性和方法可以用来获取其他信息，比如状态码、响应头、Cookies 等。示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;zhangkaiheng.gitee.io&#x2F;&#39;)</span><br><span class="line">print(type(r.status_code), r.status_code)</span><br><span class="line">print(type(r.headers), r.headers)</span><br><span class="line">print(type(r.cookies), r.cookies)</span><br><span class="line">print(type(r.url), r.url)</span><br><span class="line">print(type(r.history), r.history)</span><br></pre></td></tr></table></figure><p>这里分别打印输出 status_code 属性得到状态码，输出 headers 属性得到响应头，输出 cookies 属性得到 Cookies，输出 url 属性得到 URL，输出 history 属性得到请求历史。</p><p>运行结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &#39;int&#39;&gt; 200</span><br><span class="line">&lt;class &#39;requests.structures.CaseInsensitiveDict&#39;&gt; &#123;&#39;Date&#39;: &#39;Sat, 09 Jan 2021 12:54:40 GMT&#39;, &#39;Content-Type&#39;: &#39;text&#x2F;html&#39;, &#39;Transfer-Encoding&#39;: &#39;chunked&#39;, &#39;Connection&#39;: &#39;keep-alive&#39;, &#39;Server&#39;: &#39;openresty&#x2F;1.13.6.2&#39;, &#39;Last-Modified&#39;: &#39;Mon, 04 Jan 2021 07:53:48 GMT&#39;, &#39;ETag&#39;: &#39;W&#x2F;&quot;5ff2c98c-9df5&quot;&#39;, &#39;Expires&#39;: &#39;Sun, 10 Jan 2021 12:54:40 GMT&#39;, &#39;Cache-Control&#39;: &#39;max-age&#x3D;86400&#39;, &#39;Content-Encoding&#39;: &#39;gzip&#39;&#125;</span><br><span class="line">&lt;class &#39;requests.cookies.RequestsCookieJar&#39;&gt; &lt;RequestsCookieJar[]&gt;</span><br><span class="line">&lt;class &#39;str&#39;&gt; https:&#x2F;&#x2F;zhangkaiheng.gitee.io&#x2F;</span><br><span class="line">&lt;class &#39;list&#39;&gt; []</span><br></pre></td></tr></table></figure><p>可以看到，headers 和 cookies 这两个属性得到的结果分别是 CaseInsensitiveDict 和 RequestsCookieJar 类型。</p><p>我们知道，<a href="https://baike.baidu.com/item/HTTP%E7%8A%B6%E6%80%81%E7%A0%81/5053660?fr=aladdin">状态码</a>是用来表示响应状态的，比如返回 200 代表我们得到的响应是没问题的，上面的例子正好输出的结果也是 200，所以我们可以通过判断 Response 的状态码来确认是否爬取成功。</p><p>requests 还提供了一个内置的状态码查询对象 requests.codes，用法示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;zhangkaiheng.gitee.io&#x2F;&#39;)</span><br><span class="line">exit() if not r.status_code &#x3D;&#x3D; requests.codes.ok else print(&#39;Request Successfully&#39;)</span><br></pre></td></tr></table></figure><p>这里通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用 requests.codes.ok 得到的是成功的状态码 200。</p><p>这样的话，我们就不用再在程序里面写状态码对应的数字了，用字符串表示状态码会显得更加直观。</p><h1 id="爬虫入门首选开刀网站——猫眼电影"><a href="#爬虫入门首选开刀网站——猫眼电影" class="headerlink" title="爬虫入门首选开刀网站——猫眼电影"></a>爬虫入门首选开刀网站——猫眼电影</h1><h2 id="爬虫准备"><a href="#爬虫准备" class="headerlink" title="爬虫准备"></a>爬虫准备</h2><p>猫眼电影是美团旗下的一家集媒体内容、在线购票、用户互动社交、电影衍生品销售等服务的一站式电影互联网平台。2015年6月，猫眼电影覆盖影院超过4000家，这些影院的票房贡献占比超过90%。目前，猫眼占网络购票70%的市场份额，每三张电影票就有一张出自猫眼电影，是影迷下载量较多、使用率较高的电影应用软件。同时，猫眼电影为合作影院和电影制片发行方提供覆盖海量电影消费者的精准营销方案，助力影片票房。</p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/20210109211910.png"></p><p>运行平台：win10</p><p>Python版本：3.8</p><p>IDE：Pycharm</p><p>浏览器：Chrome</p><p>目标：获取猫眼Top100电影名称、主演、上映时间、评分、封面图片</p><p>如此网站数据看上去都心动，不过猫眼电影也是有反爬机制的，只不过今天我们的入门小打小闹触及不深。</p><h2 id="分析网页"><a href="#分析网页" class="headerlink" title="分析网页"></a>分析网页</h2><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/20210109214518.png"></p><p>通过F12或者右键选择检查我们发现两个规律：</p><ol><li>要获取的数据有规律的存在网页代码中：如标题在class属性为name的p标签里、主演在class属性为star的p标签里······</li><li>第一页链接offset=0而第二页链接offset=10推测第三页offset=20</li></ol><h2 id="请求网页"><a href="#请求网页" class="headerlink" title="请求网页"></a>请求网页</h2><p>分析完网页我们可以试着模拟浏览器请求网页</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import requests  </span><br><span class="line"></span><br><span class="line">r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;maoyan.com&#x2F;board&#x2F;4?offset&#x3D;0&#39;)  </span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>为了保证爬虫正常，添加请求头</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">headers &#x3D; &#123;</span><br><span class="line">  &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;87.7.36.5901 Safari&#x2F;537.36&#39;</span><br><span class="line">&#125;</span><br><span class="line">r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;maoyan.com&#x2F;board&#x2F;4?offset&#x3D;0&#39;, headers&#x3D;headers)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure><p>试着利用最简单的正则获取第一页的标题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">headers &#x3D; &#123;</span><br><span class="line">  &#39;User-Agent&#39;: &#39;Mozilla&#x2F;5.0 (Windows NT 10.0; Win64; x64) AppleWebKit&#x2F;537.36 (KHTML, like Gecko) Chrome&#x2F;87.7.36.5901 Safari&#x2F;537.36&#39;</span><br><span class="line">&#125;</span><br><span class="line">r &#x3D; requests.get(&#39;https:&#x2F;&#x2F;maoyan.com&#x2F;board&#x2F;4?offset&#x3D;0&#39;, headers&#x3D;headers)</span><br><span class="line">pattern &#x3D; re.compile(&#39;&lt;p class&#x3D;&quot;name&quot;&gt;&lt;a href&#x3D;&quot;&#x2F;films&#x2F;.*?&gt;(.*?)&lt;&#x2F;a&gt;&#39;, re.S)</span><br><span class="line">titles &#x3D; re.findall(pattern, r.text)</span><br><span class="line">print(titles)</span><br><span class="line"># 运行结果</span><br><span class="line"># [&#39;我不是药神&#39;, &#39;肖申克的救赎&#39;, &#39;绿皮书&#39;, &#39;海上钢琴师&#39;, &#39;哪吒之魔童降世&#39;, &#39;小偷家族&#39;, &#39;霸王别姬&#39;, &#39;美丽人生&#39;, &#39;盗梦空间&#39;, &#39;这个杀手不太冷&#39;]</span><br></pre></td></tr></table></figure><p>针对规律一我们很容易利用正则解析</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 主演</span><br><span class="line">star &#x3D; re.findall(re.compile(&#39;&lt;p class&#x3D;&quot;star&quot;&gt;(.*?)&lt;&#x2F;p&gt;&#39;, re.S), r.text)</span><br><span class="line"># 发表时间</span><br><span class="line">releasetime &#x3D; re.findall(re.compile(&#39;&lt;p class&#x3D;&quot;releasetime&quot;&gt;(.*?)&lt;&#x2F;p&gt;&#39;, re.S), r.text)</span><br><span class="line"># 封面链接</span><br><span class="line">img_urls &#x3D; re.findall(re.compile(&#39;&lt;img data-src&#x3D;&quot;(.*?)&quot; alt&#x3D;&quot;.*?&quot; class&#x3D;&quot;board-img&quot; &#x2F;&gt;&#39;, re.S), r.text)</span><br><span class="line"># 排名</span><br><span class="line">top &#x3D; re.findall(re.compile(&#39;&lt;i class&#x3D;&quot;board-index board-index-.*?&quot;&gt;(.*?)&lt;&#x2F;i&gt;&#39;, re.S), r.text)</span><br></pre></td></tr></table></figure><p>不写了。。。看<a href="https://www.bilibili.com/video/BV1Kr4y1T7z8/">视频</a>听口述吧代码自己敲别<a href="https://github.com/Kit139/maoyan_movies">copyme</a></p>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CIT </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>20210102寒假第一交流会——黑屏见翻车？不！我在检测认真质量</title>
      <link href="2021/01/02/20210102%E5%AF%92%E5%81%87%E7%AC%AC%E4%B8%80%E4%BA%A4%E6%B5%81%E4%BC%9A%E2%80%94%E2%80%94%E9%BB%91%E5%B1%8F%E8%A7%81%E7%BF%BB%E8%BD%A6%EF%BC%9F%E4%B8%8D%EF%BC%81%E6%88%91%E5%9C%A8%E6%A3%80%E6%B5%8B%E8%AE%A4%E7%9C%9F%E8%B4%A8%E9%87%8F/"/>
      <url>2021/01/02/20210102%E5%AF%92%E5%81%87%E7%AC%AC%E4%B8%80%E4%BA%A4%E6%B5%81%E4%BC%9A%E2%80%94%E2%80%94%E9%BB%91%E5%B1%8F%E8%A7%81%E7%BF%BB%E8%BD%A6%EF%BC%9F%E4%B8%8D%EF%BC%81%E6%88%91%E5%9C%A8%E6%A3%80%E6%B5%8B%E8%AE%A4%E7%9C%9F%E8%B4%A8%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="20210102寒假第一交流会——黑屏见翻车？不！我在检测认真质量"><a href="#20210102寒假第一交流会——黑屏见翻车？不！我在检测认真质量" class="headerlink" title="20210102寒假第一交流会——黑屏见翻车？不！我在检测认真质量"></a>20210102寒假第一交流会——黑屏见翻车？不！我在检测认真质量</h1><iframe id="pqvE7xEK-1596373719908" src="https://player.bilibili.com/player.html?aid=330946159" allowfullscreen="true" data-mediaembed="bilibili" __idm_id__="291592193" style="box-sizing: border-box; outline: 0px; margin: 0px; padding: 0px; font-weight: normal; overflow-wrap: break-word; display: block; width: 660px; height: 330px;"></iframe><h2 id="明晰定位，确立方向"><a href="#明晰定位，确立方向" class="headerlink" title="明晰定位，确立方向"></a>明晰定位，确立方向</h2><ul><li>Python——除了生孩子不可以其他啥都行不假但不适合任何人</li><li>编程——工管人、经济人躲不掉</li><li>编程触类旁通——学Python培养编程思维利VB</li></ul><h2 id="选课推荐"><a href="#选课推荐" class="headerlink" title="选课推荐"></a>选课推荐</h2><ul><li>数学建模</li></ul><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/20210102220412.png"></p><ul><li><p>Python程序设计</p></li><li><p>MS高级应用</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/20210102220812.png"></p><h2 id="互动环节"><a href="#互动环节" class="headerlink" title="互动环节"></a>互动环节</h2><p>主修与辅修</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>利用好github、gitee</li></ul><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/20210102221419.png"></p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/20210102221202.png"></p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/20210102220935.png"></p><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/20210102221113.png"></p><ul><li><p>作业任务</p><p>一星期过一遍Python<a href="https://www.w3school.com.cn/python/index.asp">基础知识</a>和<a href="https://www.bilibili.com/video/BV1ta411w7D3">回放</a>等</p><p>查找资料预习爬虫(可以看看<a href="https://www.bilibili.com/video/BV1NE411Q7i2">解师哥的视频</a>注意有P2) </p><p>下周六个人学习总结分享 </p><p>B站、知乎、微信公众号等开源资源要利用好</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> CIT </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CIT </tag>
            
            <tag> Python </tag>
            
            <tag> 选课 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>给hexo Butterfly扩展图标</title>
      <link href="2020/12/28/%E7%BB%99hexo%20Butterfly%E6%89%A9%E5%B1%95%E5%9B%BE%E6%A0%87/"/>
      <url>2020/12/28/%E7%BB%99hexo%20Butterfly%E6%89%A9%E5%B1%95%E5%9B%BE%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<h1 id="给hexo-Butterfly扩展图标"><a href="#给hexo-Butterfly扩展图标" class="headerlink" title="给hexo Butterfly扩展图标"></a>给hexo Butterfly扩展图标</h1><p>由于hexo butterfly主题采用了Font Awesome图标（不包含如B站，知乎这类中国大陆网站的图标）所以如果需要使用的话需要额外自行添加</p><h2 id="Font-Awesome相关知识"><a href="#Font-Awesome相关知识" class="headerlink" title="Font Awesome相关知识"></a><a href="http://www.fontawesome.com.cn/">Font Awesome</a>相关知识</h2><p>引用<a href="https://zh.wikipedia.org/wiki/Font_Awesome">维基百科</a>中的介绍</p><blockquote><p>Font Awesome 是一个基于CSS和LESS的字体和图标工具套件。它由Dave Gandy制作，用于Twitter Bootstrap，后来被整合到BootstrapCDN 中。Font Awesome在使用第三方Font Scripts的网站中占有20％的市场份额，排在Google字型之后的第二位。</p></blockquote><p>简单的说，Font Awesome是一个图标库，当你的博客需要使用一个图标时（比如）</p><p>由于next已经使用了Font Awesome，所以只需要插入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;i class&#x3D;&quot;fab fa-github&quot;&gt;&lt;&#x2F;i&gt;</span><br></pre></td></tr></table></figure><p>即可。具体的Font Awesome使用方法可以看<a href="https://www.runoob.com/font-awesome/fontawesome-tutorial.html">菜鸟教程</a></p><h2 id="第三方图标库的使用"><a href="#第三方图标库的使用" class="headerlink" title="第三方图标库的使用"></a>第三方图标库的使用</h2><p>由于Font Awesome图标不包含如B站，知乎这类中国大陆网站的图标，所以如果需要使用的话需要额外自行添加第三方的图标库，在这里，我们使用使用<a href="https://www.iconfont.cn/">阿里巴巴矢量库</a></p><h3 id="下载图标"><a href="#下载图标" class="headerlink" title="下载图标"></a>下载图标</h3><p>首先，我们在阿里巴巴图标库挑选需要的图标，具体可以使用搜索功能进行搜索，在图标上点击添加入库将其加入库中，之后点击右上方购物车图标，点击下载代码获得一个压缩文件</p><h3 id="使用下载的图标"><a href="#使用下载的图标" class="headerlink" title="使用下载的图标"></a>使用下载的图标</h3><p>打开文件中的<code>iconfont.css</code>，将其中的内容（比如这样）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">@font-face &#123;font-family: &quot;iconfont&quot;;</span><br><span class="line">    src: url(&#39;iconfont.eot?t&#x3D;1589906807175&#39;); &#x2F;* IE9 *&#x2F;</span><br><span class="line">    src: url(&#39;iconfont.eot?t&#x3D;1589906807175#iefix&#39;) format(&#39;embedded-opentype&#39;), &#x2F;* IE6-IE8 *&#x2F;</span><br><span class="line">    url(&#39;data:application&#x2F;x-font-woff2;charset&#x3D;utf-8;base64,d09GMgABAAAAAAiAAAsAAAAAD0AAAAgzAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHEIGVgCDNgqRPI4HATYCJAMYCw4ABCAFhG0HXxuyDCMR9oOSQpP9RYJ5yE75AyR1CmZsFOu9SJ3BIn3W9BPb&#x2F;Ad3h9hYq2CuK9yPrpD4rxP35f5dCEIazng0oUypUg1gxStcoQJ2sK5Mrk5aBxUGhfHrIJXedPa780X2JD+rwebUuibgQmWZsHTyyT3kZCsny23&#x2F;A+BgzdWJJ+8bcJj28nLIzRxTTeQlQiYkEW&#x2F;&#x2F;JGskqBAKIUYqObPoZoWJJy1mwQMT&#x2F;bqbAQVAxIILao1LyQEWBnZKDng01lcCWwjALQYK62aIEwuoNxDsNIo+CwATou+n&#x2F;uAVFoBGGsCBGXWxNRDxXKTXI6mXSgIJIF8xAKxPA0AAXABgQF1kaQ2ghFwih4jqjgUAYHEzGjwXP5&#x2F;9olSv1ypR&#x2F;qMUs0TqHx4ADUMIoAaAQ6KUKW0EPBdZfJEofYBPztkyHSDwojQNEPKcLPhoHoax2hcgWYaeAB4X&#x2F;qE8RAAXZ44yVIjdDS0kao0NQxQsVsVKoqptNiISVtb8uVguQCAwl0hEQmNDB04q7SKhpMVjzgG+L4pkoy9TpJqZ39hZBpc6ecvm3EXCXe3nSF0bQXsLdoZsr+0gsOxTD6rxV0d2N&#x2F;Z6zcbWiga7qssQ1XpQX5xzc6t7jtZoPiE0oDcqYtVpXdmjYQgV7z7icaUyITtaY9SU7FK9Zu3007LNqHJbYYMx2RvaKBVK8dpZPX5Zra&#x2F;vOVxVtzrXrFebWHEqfaPraM1IReuuwdQmASUh8zVWy&#x2F;djiKC1AARiCY&#x2F;XJHCgmUqHgS+gfDjFlkAixLAHIVnLS75d3A+f8bSUVyOobdMEE6IUAUs0pHOaDm&#x2F;ad0AM&#x2F;NupIkxx7cNo8sojtetmG5evk5q1OU3kORbOEpfdMQL4FGYLdsMA5UjQC++rNGSIwyGO+CgOZ1ApxEfmYzP+yA2kNXY1wPfHSpeI&#x2F;Jv4vWp8glc3&#x2F;J2qMGouX4ZYvZvFIoy2N0VEersaO6Fw0IxWFQEcRHYUkSC+9TvljkrXNly8fVrcf4Adgec9QLaL5uF3NYWqc+9IRLoK1pX+68qYQ2gcO3Eq0uRFlLhpzOSLo+gepfeLdzzBucnErmrINJq19bJO+GGoVBD4zxUshEniZutk&#x2F;diXkauhX19dy5GrYEVyNVtXj7v&#x2F;bpAEE1nv1XL4zcFCefWS2NhzxFP7NgF9b9x78rcV8vT+zRQatOXAGbE4b&#x2F;qgNWMqZUt3GtED19m2rthjigbtr1i2CmDgytGrKaplWXXuefb2HHBgwEaKt9TeseH4ezzY&#x2F;LRy3nHAqIhRUQPDIXz&#x2F;vzG0LdZ3RNLKIumJ+AWqN9KmFLfdXyC0bWPUZlggPOl26a2ZIIFJveRidCc8Uzrsg31Ps+Gd4i4Lqx5l6Mat7M5bR3tY2XpoC+p4z0tfJwvyDdJSC7lYWu56ZLPseGQB2z1ydmxQiYHHmyB+8Jfbmb65gVGZ9vYDD90xE7f5C&#x2F;LYXDrX6tiSK7Gik3KK97+km&#x2F;QzusBqKG9l35GhtDh38eHtadyI0g4mQZL0AeZ+1&#x2F;6YO8ZYazR3srFYhVVCBbEnCqEaq8XG2L+QxR5KARbIWKzBrCwRQFmBvfy7DlNymBNjsQIrGLUqxpufldDVe3ipWiBgMIqKJLeE1j6+Xn5+ym48vwfRhgwOxpbqDMywWJBV46jGtDWNTaREKrU5z2Ap42yNrRlxNcgI2cdE62x0MTFVNlXRMYacROPjs3e0xoSYcIQzJab+PWLssyOTil0NxwlkZEqQyoAYKBGJV8qVTiq5app5fGS8pxkRe8zqhn1It24aG&#x2F;&#x2F;Ra9zETiaus2ZGq4iWEDnBWszII4nUkXGUMdamuO6RvpbYxNsLjyjq5xkfqeXrvaQKcxmRKYlSTuTmz+RXTfHxJm5O7thnBv8vL98rv+6&#x2F;9&#x2F;uRw7yn9QSrDW0Y&#x2F;slj4Z1FeEMO49zVI4JLLU44Gkxpv76aRacguB&#x2F;3cKOvJgKfwZYjbeNdYuqKPFTZkqbEqDXCxaexoGSV0xHjKWY973SlpkYZZnZmLpmWkJxakaRJUlM34WHXbl5R5ps29uVzjI36aNOLTSschk108E2RVVmr+12qbCfdBJ+KvStsKwryS0J0aEYplYwfZkatjulAMzLpJDplum6CQaPackA+te5ylHUPsxAb&#x2F;5FTTkZu0c6L1G3T7o6Mr3nY1&#x2F;3Po77pP54gwqtp3bRpOloeYxnzMJNqaaEyFdYg68fa0qhOnggssrIfKmRSRQTgTig63shIZgqgWkSjRo00P3qWn8WfTU0ttKibVWuRZ1E7q87C6VJSmJrauuWHN04T3R1KjD9Pdge9qan3nf7cCGWucqS2Z9Dcox64V1TcUztS1oyAvgCH6He0wygYAAB9afNi8zG0G&#x2F;nq1t9oH62nyj&#x2F;I0FHaCQyP7Ffl8pBvrPh&#x2F;as3t1249dK1+RwEP77Jgb3SCsOcSfCqAIFOHmS3fK57JIeDLaeDmhPUAlUr3ucIavO0UliYDmoA5ICytFCtdgBDzAQYrHEScxZ8uptIJCFVCAHAyTACFswFoMgcA4ZyTYuU9IAy8AwYXBCJ5lOqCYgsc0rg5MSpBC&#x2F;oPKkO9M2HZxAPvWIRWcb4gsr6RfdUAWZyWO27YI3Mc4j9FLuLAMXVwFbfDtiUYmGo0EpciwzFJHHVobKgbbZwYlQQpLdB&#x2F;eStDvYstm8rX37EIreKWGY9538i+Wj&#x2F;IxNIe0puj7zXjrQz3nyInqtiBbWbqEtdVF2OLrQQGersajcTKEc3DUaJWcn0d8fKu7lt2+PpmBbFGprRUekZmi5OCVv3MeNvP&#x2F;8uqDHsJulKUWf8DRXt&#x2F;W9miyNT0OBoBAA&#x3D;&#x3D;&#39;) format(&#39;woff2&#39;),</span><br><span class="line">    url(&#39;iconfont.woff?t&#x3D;1589906807175&#39;) format(&#39;woff&#39;),</span><br><span class="line">    url(&#39;iconfont.ttf?t&#x3D;1589906807175&#39;) format(&#39;truetype&#39;), &#x2F;* chrome, firefox, opera, Safari, Android, iOS 4.2+ *&#x2F;</span><br><span class="line">    url(&#39;iconfont.svg?t&#x3D;1589906807175#iconfont&#39;) format(&#39;svg&#39;); &#x2F;* iOS 4.1- *&#x2F;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  .iconfont &#123;</span><br><span class="line">    font-family: &quot;iconfont&quot; !important;</span><br><span class="line">    font-size: 16px;</span><br><span class="line">    font-style: normal;</span><br><span class="line">    -webkit-font-smoothing: antialiased;</span><br><span class="line">    -moz-osx-font-smoothing: grayscale;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  .icon-douban:before &#123;</span><br><span class="line">    content: &quot;\f01c8&quot;;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  .icon-csdn:before &#123;</span><br><span class="line">    content: &quot;\e60a&quot;;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  .icon-zhihu:before &#123;</span><br><span class="line">    content: &quot;\e69a&quot;;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  .icon-tubiaozhizuo-:before &#123;</span><br><span class="line">    content: &quot;\e60b&quot;;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  .icon-bilibili-line:before &#123;</span><br><span class="line">    content: &quot;\e75d&quot;;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>全部复制到<code>\blogs\themes\next\source\lib\font-awesome\css\all.min.css</code>后边。这样，就可以使用新添加的图标了</p><p>：<code>&lt;i class=&quot;iconfont icon-bilibili-line&quot;&gt;&lt;/i&gt;</code></p><h3 id="在侧栏的社交链接中加入bilibili"><a href="#在侧栏的社交链接中加入bilibili" class="headerlink" title="在侧栏的社交链接中加入bilibili"></a>在侧栏的社交链接中加入bilibili</h3><p>打开_config.butterfly.yml或者_config.yml文件，搜索找到social：增加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bilibili: https:&#x2F;&#x2F;space.bilibili.com&#x2F;B站id || iconfont icon-bilibili-line</span><br></pre></td></tr></table></figure><p>修改完如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">social:</span><br><span class="line">  #GitHub: https:&#x2F;&#x2F;github.com&#x2F;yourname || fab fa-github</span><br><span class="line">  #E-Mail: mailto:yourname@gmail.com || fa fa-envelope</span><br><span class="line">  #Weibo: https:&#x2F;&#x2F;weibo.com&#x2F;yourname || fab fa-weibo</span><br><span class="line">  #Google: https:&#x2F;&#x2F;plus.google.com&#x2F;yourname || fab fa-google</span><br><span class="line">  #Twitter: https:&#x2F;&#x2F;twitter.com&#x2F;yourname || fab fa-twitter</span><br><span class="line">  #FB Page: https:&#x2F;&#x2F;www.facebook.com&#x2F;yourname || fab fa-facebook</span><br><span class="line">  #StackOverflow: https:&#x2F;&#x2F;stackoverflow.com&#x2F;yourname || fab fa-stack-overflow</span><br><span class="line">  #YouTube: https:&#x2F;&#x2F;youtube.com&#x2F;yourname || fab fa-youtube</span><br><span class="line">  #Instagram: https:&#x2F;&#x2F;instagram.com&#x2F;yourname || fab fa-instagram</span><br><span class="line">  #Skype: skype:yourname?call|chat || fab fa-skype</span><br><span class="line">  bilibili: https:&#x2F;&#x2F;space.bilibili.com&#x2F;B站id || iconfont icon-bilibili-line</span><br></pre></td></tr></table></figure><p>把B站id换成自己的B站id即可</p><p>参考文章：<a href="https://x-zeppelin.github.io/hexo-icon/">给hexo next扩展图标</a></p>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> 图标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>同济校友世纪互联白票</title>
      <link href="2020/12/27/%E5%90%8C%E6%B5%8E%E6%A0%A1%E5%8F%8B%E4%B8%96%E7%BA%AA%E4%BA%92%E8%81%94%E7%99%BD%E7%A5%A8/"/>
      <url>2020/12/27/%E5%90%8C%E6%B5%8E%E6%A0%A1%E5%8F%8B%E4%B8%96%E7%BA%AA%E4%BA%92%E8%81%94%E7%99%BD%E7%A5%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="同济校友世纪互联白票"><a href="#同济校友世纪互联白票" class="headerlink" title="同济校友世纪互联白票"></a><strong>同济校友世纪互联白票</strong></h1><p>社工库免费查询：@sgk123bot  </p><p>身份户籍、手机机主、开房记录、快递地址、贷款记录、车牌车主、个人常用密码、QQ/邮箱/微博/网络账号、就职单位和银行开户等联系方式。  </p><p>【每日1次免费】 输入推广码: IVT51C7A00B 额外获得5次查询。   </p><p>学号姓名怎么获取已经知道了，tg某社工库机器人库比较全。</p><p>之所以选生僻字也是这一点，缩小范围，再根据学号可以基本确定sfz里的出生年份进一步确定姓名对应的sfz号。  <a href="https://mail-mgmt.tongji.edu.cn/regOfficialEmail/checkPhone">https://mail-mgmt.tongji.edu.cn/regOfficialEmail/checkPhone</a> 同济大学的校友邮箱需要两个信息： 1.学工号  2.身份证号 </p><p> <img src="https://gitee.com/cit_k/pictures/raw/master/picture/Dgj2K7OYsc6UA8L.png" alt="img"> </p><p>第一步： 搜索学生名单，获取已经毕业了的学生学号  google： 同济大学 学生名单 ，或者直接去同济公开网查  <a href="http://xxgk.tongji.edu.cn/index.php?classid=3148&amp;page=5">http://xxgk.tongji.edu.cn/index.php?classid=3148&amp;page=5</a> </p><p>第二步： 去表格里找 生僻的名字 <img src="https://gitee.com/cit_k/pictures/raw/master/picture/RY9xaQkBq6zeHt7.png" alt="img"> 第三步：上TG去 @sgk123bot 搜索生僻的名字， 多试几个 ，有免费条数，精髓在于只搜索不查看的话 不消耗条数</p><p>  <img src="https://gitee.com/cit_k/pictures/raw/master/picture/SCpGQFNVdqXyack.png" alt="img"> </p><p>第四步： 找到只有一条记录的，如我找的这个  <img src="https://gitee.com/cit_k/pictures/raw/master/picture/jZC3WFONt9sBTHk.png" alt="img"> </p><p>第五步： 回到校友邮箱，输入他的学工号，你的手机号 进行注册，让你验证身份证就输入他的，搞定。    </p><p>第六步： 拿到邮箱以后github上找 onemanager 这个项目，用heroku搭建列表程序(同济可以用API但不能创建API，onemanager自带同济API，是最简单的) </p><p> 第七步： 搭建完成后配置邮箱时会自动弹窗授权，点击确定后在下面这个图的地方什么都不选，点确认就OK了  <img src="https://gitee.com/cit_k/pictures/raw/master/picture/duwWgDxq42VLXlU.png" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 白票 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 黑科技 </tag>
            
            <tag> 白票 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gitkarken Pro版本的破解使用</title>
      <link href="2020/12/25/Gitkarken%20Pro%E7%89%88%E6%9C%AC%E7%9A%84%E7%A0%B4%E8%A7%A3%E4%BD%BF%E7%94%A8/"/>
      <url>2020/12/25/Gitkarken%20Pro%E7%89%88%E6%9C%AC%E7%9A%84%E7%A0%B4%E8%A7%A3%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="Gitkarken-Pro版本的破解使用"><a href="#Gitkarken-Pro版本的破解使用" class="headerlink" title="Gitkarken Pro版本的破解使用"></a>Gitkarken Pro版本的破解使用</h2><h3 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h3><ul><li><a href="https://www.gitkraken.com/download">官方地址</a></li><li>历史版本稳定：<a href="https://release.axocdn.com/win64/GitKrakenSetup-6.5.0.exe">GitKraken-6.5.0</a></li></ul><p><img src="https://cdn.jsdelivr.net/gh/CIT-K/pictures/picture/170c816e735f6251" alt="img"></p><h3 id="屏蔽更新host"><a href="#屏蔽更新host" class="headerlink" title="屏蔽更新host"></a>屏蔽更新host</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># gitKraken 更新屏蔽</span><br><span class="line">127.0.0.1 release.gitkraken.com</span><br></pre></td></tr></table></figure><h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><p>需要安装<code>node.js</code>和<code>yarn</code>，其中<code>node.js</code>安装时要选择加入系统环境中，<code>yarn</code>在安装后也要手动进行添加进系统环境。</p><h3 id="打开gitkraken并登陆"><a href="#打开gitkraken并登陆" class="headerlink" title="打开gitkraken并登陆"></a>打开gitkraken并登陆</h3><p>软件右下角有显示版本和Free(功能受限)</p><h3 id="下载破解脚本并破解"><a href="#下载破解脚本并破解" class="headerlink" title="下载破解脚本并破解"></a>下载破解脚本并破解</h3><p>这里使用<a href="https://github.com/5cr1pt/GitCracken">GitCracken</a></p><p>本地clone下载</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/<span class="number">5</span>cr1pt/GitCracken.git</span><br></pre></td></tr></table></figure><p>破解</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> GitCracken/GitCracken</span><br><span class="line">rm yarn.lock</span><br><span class="line">yarn install</span><br><span class="line">yarn build</span><br><span class="line"># 最后一个参数是此程序安装位置下的一个文件</span><br><span class="line"># windows gitbash</span><br><span class="line">node dist/bin/gitcracken.js patcher --asar ~/AppData/Local/gitkraken/app-<span class="number">6</span>.<span class="number">5</span>.<span class="number">0</span>/resources/app.asar</span><br><span class="line"># mac </span><br><span class="line">node dist/bin/gitcracken.js patcher --asar 你的gitkraken的目录/resources/app.asar</span><br></pre></td></tr></table></figure><h3 id="重启gitkraken则能看见右下角的Pro"><a href="#重启gitkraken则能看见右下角的Pro" class="headerlink" title="重启gitkraken则能看见右下角的Pro"></a>重启gitkraken则能看见右下角的Pro</h3><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/170c80eee33fc9c1" alt="img"></p><p>切换账号则重新执行node命令</p><h3 id="破解中遇到的问题"><a href="#破解中遇到的问题" class="headerlink" title="破解中遇到的问题"></a>破解中遇到的问题</h3><ol><li><p>破解最后一步使用的文件参数，若是失败的话改为自己文件的绝对位置</p></li><li><p>安装<code>node.js</code>和<code>yarn</code>之后重启，使用<code>yarn --version</code>验证是否能够使用</p></li><li><p>成功破解之后，是在<code>app.asar</code>做的修改，若是你之前打开了程序并且登录了<code>Github</code>其中保留的有你的信息，所以若是更换账户需要重新破解一次</p></li><li><p>我在最后一步破解之后，打开一直卡在启动界面上，我把<code>app.asar</code>拷贝出来重新安装了一遍，然后替换文件<code>app.asar</code>重新打开，在右下角看到<code>Pro</code>提示，若是显示是<code>Free</code>直接点击应该会变换。</p></li><li><p>在当前（2020.12.25）使用的7.0.1版本仍是可以破解的，但是为了之后保持能用最好不要更新。由于系统图标指向的是<code>Update.exe</code>程序，可在快捷方式中进行修改指向程序的真正位置。另外在<code>hosts</code>中加入<code>127.0.0.1 release.gitkraken.com</code>屏蔽更新。关于此程序的右键功能，在知乎有看大佬提到说需要修改注册表，我是直接将其关闭了</p></li></ol><p>参考文章:</p><ul><li><a href="https://juejin.cn/post/6844904087004135432">gitKraken 6.5.0 免收费破解</a></li><li><a href="https://hbaaa.github.io/2020/07/08/Gitkarken%E7%A0%B4%E8%A7%A3/">Gitkarken破解</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 破解 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 破解 </tag>
            
            <tag> 黑科技 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第一篇Hexo blog文章——Hexo + Gitee 部署个人博客</title>
      <link href="2020/12/24/%E7%AC%AC%E4%B8%80%E7%AF%87Hexo%20blog%E6%96%87%E7%AB%A0%E2%80%94%E2%80%94Hexo%20+%20Gitee%20%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>2020/12/24/%E7%AC%AC%E4%B8%80%E7%AF%87Hexo%20blog%E6%96%87%E7%AB%A0%E2%80%94%E2%80%94Hexo%20+%20Gitee%20%E9%83%A8%E7%BD%B2%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="第一篇Hexo-blog文章——Hexo-Gitee-部署个人博客"><a href="#第一篇Hexo-blog文章——Hexo-Gitee-部署个人博客" class="headerlink" title="第一篇Hexo blog文章——Hexo + Gitee 部署个人博客"></a>第一篇Hexo blog文章——Hexo + Gitee 部署个人博客</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>小组的谢老师给我们寒假布置了任务——建博客。害，其实博客这玩意儿，也就是那种“三分钟热度”，这里需要用数据说话：小组的金鼎师哥给我们培训前端展示过自己的博客，印象当中文章大多是生活感言建博之处，文章越往后时间越密集越往前越稀疏啊哈哈哈哈哈哈最后就看不到博客了(也不知是换域名了还是服务器关了)；添昊师哥<a href="https://loner1024.top/">博客</a>底子足初中就开始建了，开了又关关了又开，最近考研文章不高产了[笑哭]，但仍是我等楷模；倩师姐的博客太难见了，我当时从师哥那儿知道博客后就就就看了一眼啊一眼，啪，就没了。。。我，有一种预感，这经历会似曾相识！</p><blockquote><p>2021.1.15更新，今天才知道金鼎师哥原来是博客域名到期了，用的是<a href="http://typecho.org/">typecho</a>搭建的；添昊师哥是部署在github应该双路线在coding上也部署了不然没那么快，jekyll框架+H2O主题；倩师姐也是在github上弄得，机智的我那她的仓库重现一下结果里面没文章[好吧师姐更机智]，但可以看出是Hexo框架，同时也发现了被灰藏多年的公众号哈哈哈哈哈哈</p></blockquote><p>其实我去年就在github建了博客，科学上网的原因速度慢、百度难收录等诟病，然后就没管了。。。放了将近一年该臭了[笑哭]反正是不想再碰了</p><p>目前市场上比较火的一些博客框架： <strong>Hexo、jekyll、Solo、Halo 、gohugo、VuePress、wordpress</strong> 等等 ，这些都是开源的静态博客框架（没有登录注册，后台管理等等）好处就是能够非常快速的搭建好自己的个人博客（也是要一定前端知识的），但是你若是部署到服务器也是需要票子不断维护的Pass！Next！部署到github白票国外服务器又走了老路Pass！那有没有更好的办法呢？</p><p>当然，而且有很多，但我选择了Hexo + Gitee：</p><ul><li>Hexo富含了丰富的主题和扩展包，很大众</li><li>Gitee码云，国内代码托管平台速度快，我很爱国嘿嘿嘿</li></ul><hr><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802200250982.png" alt="在这里插入图片描述"></p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p><strong>1、安装 Node.js 环境：</strong> 因为 Hexo 是基于Node.js 的博客框架，就像 Java 要依赖 JDK 环境一样。</p><ul><li>node下载地址：<a href="http://nodejs.cn/download/">http://nodejs.cn/download/</a> ，傻瓜式安装，这里不再详述</li></ul><p><strong>NodeJS环境安装重点拓展：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确认node.js安装成功：运行下面两个命令打印版本号即可</span></span><br><span class="line">node -v</span><br><span class="line">npm  -v</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装 Node.js 淘宝镜像加速器 （cnpm）</span></span><br><span class="line">npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改 node 类库默认安装位置</span></span><br><span class="line"><span class="comment"># 默认是在 C:\Users\lingStudy\AppData\Roaming\npm</span></span><br><span class="line"></span><br><span class="line">npm config <span class="built_in">set</span> prefix <span class="string">&quot;D:\nodejs安装路径\node_global&quot;</span></span><br><span class="line">npm config <span class="built_in">set</span> cache <span class="string">&quot;D:\nodejs安装路径\node_cache&quot;</span></span><br><span class="line"><span class="comment"># 查看修改是否成功</span></span><br><span class="line">npm root -g</span><br><span class="line"><span class="comment"># 然后把D:\install\node\node_global配置到环境变量的 PATH 下即可</span></span><br></pre></td></tr></table></figure><p><strong>2、安装版本控制工具 Git ：</strong> 用来将本地项目托管到码云，所以还需要自己注册一个码云的账号</p><p>下载地址：<a href="https://git-scm.com/download">https://git-scm.com/download</a> 学程序的，在工作中 Git 是必知必会的，还没学的建议去看看，所以这里不再详细介绍 Git</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Git一些重要配置</span></span><br><span class="line">git config --global user.name <span class="string">&quot;lxxxxdy&quot;</span>  <span class="comment">#码云用户名</span></span><br><span class="line">git config --global user.email <span class="string">&quot;xxx83@qq.com&quot;</span>   <span class="comment">#邮箱</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成SSH公钥并添加到码云，实现免密码登录</span></span><br><span class="line"><span class="comment"># 1、生成公钥</span></span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line"><span class="comment"># 2、进入 C:\Users\主机名\.ssh 目录，把 id_rsa.pub 里面的信息复制到码云的 SSH公钥 中即可</span></span><br></pre></td></tr></table></figure><p><strong>Hexo 官网：</strong> <a href="https://hexo.io/zh-cn/">https://hexo.io/zh-cn/</a><br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802200636724.png" alt="在这里插入图片描述"></p><p><strong>3、Hexo安装</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 全局安装</span></span><br><span class="line">npm install hexo-cli -g</span><br><span class="line"><span class="comment"># 查看hexo版本</span></span><br><span class="line">hexo -v</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802200739294.png" alt="在这里插入图片描述"></p><h2 id="创建本地博客站点"><a href="#创建本地博客站点" class="headerlink" title="创建本地博客站点"></a>创建本地博客站点</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化一个项目，hexoblog 是项目名</span></span><br><span class="line">hexo init  hexoblog</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802200820716.png" alt="在这里插入图片描述"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、进入hexoblog项目目录</span></span><br><span class="line"><span class="built_in">cd</span> hexoblog</span><br><span class="line"><span class="comment">#/2、安装 hexoblog 项目的依赖包</span></span><br><span class="line">npm install</span><br><span class="line"><span class="comment">#/3、启动 hexoblog 项目服务</span></span><br><span class="line">hexo server</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802200859628.png" alt="在这里插入图片描述"><br>此时，访问 <a href="http://localhost:4000/">http://localhost:4000/</a> 即可看到 hexo 默认的页面和一篇“Hello World”默认生成的文章<br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802200937295.png" alt="在这里插入图片描述"><br>至此，Hexo 项目搭建成功！</p><blockquote><p>项目主要文件目录介绍：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── .deploy       <span class="comment"># 需要部署的文件</span></span><br><span class="line">├── node_modules  <span class="comment"># 项目所有的依赖包和插件</span></span><br><span class="line">├── public        <span class="comment"># 生成的静态网页文件</span></span><br><span class="line">├── scaffolds     <span class="comment"># 文章模板</span></span><br><span class="line">├── <span class="built_in">source</span>        <span class="comment"># 博客正文和其他源文件等都应该放在这里</span></span><br><span class="line">|   ├── _drafts   <span class="comment"># 草稿</span></span><br><span class="line">|   └── _posts    <span class="comment"># 文章</span></span><br><span class="line">├── themes        <span class="comment"># 主题</span></span><br><span class="line">├── _config.yml   <span class="comment"># 全局配置文件</span></span><br><span class="line">└── package.json  <span class="comment"># 项目依赖信息</span></span><br></pre></td></tr></table></figure><h2 id="新建一篇blog文章"><a href="#新建一篇blog文章" class="headerlink" title="新建一篇blog文章"></a>新建一篇blog文章</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建一篇名为 第一篇Hexo blog文章 的文章</span></span><br><span class="line">hexo new <span class="string">&quot;第一篇Hexo blog文章&quot;</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201048569.png" alt="在这里插入图片描述"><br>  之后，就可以直接编辑刚刚新建的文章，再次启动服务查看效果，如下，这里有一个问题，Hexo框架文章中的图片只支持外链的形式，有很多方法解决，比如，可以在码云或者七牛云部署一个自己的图床。<br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201113758.png" alt="在这里插入图片描述"></p><h2 id="将项目部署到码云"><a href="#将项目部署到码云" class="headerlink" title="将项目部署到码云"></a>将项目部署到码云</h2><p>1、在码云新建一个仓库，注意标红的地方，这里我的码云用户名为 <strong>lingstudy</strong><br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201146107.png" alt="在这里插入图片描述"><br>2、在项目根目录下安装git部署插件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 之后就可以使用 hexo deploy（或简写 hexo d）将项目部署到gitee远程仓库</span></span><br><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201219532.png" alt="在这里插入图片描述"><br>3、修改项目配置文件：<strong>_config.yml</strong>，在最下面，修改如下内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Deployment</span></span><br><span class="line"><span class="comment">## Docs: https://hexo.io/docs/deployment.html</span></span><br><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: git</span><br><span class="line">  repo: 仓库URL地址</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/2020080220131539.png" alt="在这里插入图片描述"><br>4、使用命令 <strong>hexo d</strong> 将项目部署到 gitee 远程仓库，此时本地文件夹中出现有一个public文件夹。<br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201344312.png" alt="在这里插入图片描述"><br>5、开启 Gitee Pages 静态网页托管服务<br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201407831.png" alt="在这里插入图片描述"><br>之后使用提供的网址即可访问博客，每次重新上传代码到gitee时，都需要点击下图的更新按钮<strong>重启page服务</strong><br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/202008022014338.png" alt="在这里插入图片描述"><br>访问生成的网址 <a href="https://lingstudy.gitee.io/">https://lingstudy.gitee.io/</a> ，部署成功！<br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201510794.png" alt="在这里插入图片描述"></p><h2 id="修改主题"><a href="#修改主题" class="headerlink" title="修改主题"></a>修改主题</h2><p>官网主题：<a href="https://hexo.io/themes/">官方主题</a><br>可以去官网找自己喜欢的主题，下载下来，我这里就随便拿一个来演示了</p><p>1、进入所下载主题根目录下的 theme 目录，将里面的文件复制到自己项目的 theme 目录下<br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/2020080220154554.png" alt="在这里插入图片描述"><br>2、修改根目录下的配置文件 <strong>_config.yml</strong><br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201608536.png" alt="在这里插入图片描述"><br>3、启动项目，访问：<a href="http://localhost:4000/">http://localhost:4000/</a> 先在本地查看主题是否修改成功<br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201649173.png" alt="在这里插入图片描述"><br>4、将修改后的项目部署到远程仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成静态网站文件</span></span><br><span class="line">hexo g  </span><br><span class="line"><span class="comment"># 上传到远程仓库</span></span><br><span class="line">hexo d  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、清除 hexo 的缓存</span></span><br><span class="line">hexo clean</span><br><span class="line"><span class="comment"># 2、采用一键部署</span></span><br></pre></td></tr></table></figure><p><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201726963.png" alt="在这里插入图片描述"><br>5、访问 <a href="https://lingstudy.gitee.io/">https://lingstudy.gitee.io/</a> 主题样式修改并部署成功！</p><p><strong>（这里若出现样式错乱，可能是浏览器缓存问题，执行 Ctrl + F5 强制刷新一下即可）</strong><br><img src="https://gitee.com/cit_k/pictures/raw/master/picture/20200802201812780.png" alt="在这里插入图片描述"><br>至此，Hexo + Gitee 部署自己的个人博客完成！</p><h2 id="拓展：快速编写文章"><a href="#拓展：快速编写文章" class="headerlink" title="拓展：快速编写文章"></a>拓展：快速编写文章</h2><p>新建文章时每次都要执行 <strong>hexo new “blogName”</strong> 命令很麻烦</p><p>可以直接到根目录 /source/_posts 目录下，创建一个.md 文件进行编写，在文件的顶部添加下面内容就可以了，主要写一个 title 就好了，其他都可以省略</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章名称</span><br><span class="line">date: 2020-07-30 16:46:07(创建时间)</span><br><span class="line">tags: 标签名</span><br><span class="line">categories: 分类</span><br><span class="line">description: 描述</span><br><span class="line">comments: 是否开启评论(<span class="literal">true</span> or <span class="literal">false</span>)</span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>之后，使用 <strong>hexo g –d</strong> 命令推送到 gitee上，并在 gitee 上更新一下 Gitee Pages 服务即可</p><hr><h2 id="END"><a href="#END" class="headerlink" title="END"></a>END</h2><p>步骤不是太复杂，思路清晰就行，但不得不说还是麻烦，特别是主题的选择配置要花费非常多的时间（强迫症的就更多了）<br>我的博客，可能，要重走师哥师姐博客的路了。。。也许，师弟师妹也是</p><p>参考文章：</p><ul><li><a href="https://blog.csdn.net/weixin_42365530/article/details/107750003">Hexo+gitee：30分钟搭建一个自己的个人博客网站</a></li><li><a href="https://butterfly.js.org/posts/21cfbf15/">Butterfly 安装文档(一) 快速开始</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
